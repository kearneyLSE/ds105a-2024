{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style='font-size:1.5em'>**üßë‚Äçüè´ Week 08 Lecture**</font><br>\n",
    "<font style='font-size:1.3em;color:#888888'>NOTEBOOK 02: Normalising and storing data to a database</font>\n",
    "\n",
    "<font style='font-size:1.2em;color:#e26a4f;font-weight:bold'>LSE DS105A ‚Äì Data for Data Science (2024/25) </font>\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"color: #333333; background-color:rgba(226, 106, 79, 0.075); border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); padding: 20px 0 20px 10px; margin: 10px 0 10px 0; flex: 1 1 calc(45% - 20px);min-width: 250px;max-width: 350px;align-items:top;min-height: calc(45% - 20px); box-sizing: border-box;font-size:0.9em;\">\n",
    "\n",
    "üóìÔ∏è **DATE:** 21 November 2024 \n",
    "\n",
    "‚åö **TIME:** 16.00-18.00\n",
    "\n",
    "üìç **LOCATION:** CLM.5.02\n",
    "</div>\n",
    "\n",
    "\n",
    "**AUTHORS:**  Dr. [Jon Cardoso-Silva](https://jonjoncardoso.github.io)\n",
    "\n",
    "**DEPARTMENT:** [LSE Data Science Institute](https://lse.ac.uk/dsi)\n",
    "\n",
    "**OBJECTIVE**: In Weeks 08 and 09, we will revisit the data science workflow (collection -> storage -> processing -> analysis -> visualization) but with new tools and techniques. We will cover how to use APIs that require authentication to collect data, we will revisit the notion of API endpoints and then, once we have collected the data, we will learn how to store it in a more structured way using databases.\n",
    "\n",
    "<details style=\"width:70%;font-size:1em;border: 1px solid #aaa;border-radius: 4px;padding: .5em;margin-left:0em\"><summary style=\"font-weight:bold\">üñáÔ∏è EXPAND FOR USEFUL LINKS</summary>\n",
    "\n",
    "- Python 3's [`venv` module documentation](https://docs.python.org/3/library/venv.html)\n",
    "\n",
    "- W3 Schools' [HTTP Request Methods](https://www.w3schools.com/tags/ref_httpmethods.asp) page\n",
    "\n",
    "- [Reddit API documentation](https://www.reddit.com/dev/api/)\n",
    "- [Reddit API Rules](https://support.reddithelp.com/hc/en-us/articles/16160319875092-Reddit-Data-API-Wiki)\n",
    "\n",
    "- The [JSON Crack Extension](https://marketplace.visualstudio.com/items?itemName=AykutSarac.jsoncrack-vscode) for VS Code to visually inspect JSON files.\n",
    "\n",
    "- üêº pandas' [`pd.json_normalize()` function documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html)\n",
    "\n",
    "- [The `pydotenv` library](https://pypi.org/project/python-dotenv/)\n",
    "- [What is the gitignore file?](https://www.atlassian.com/git/tutorials/saving-changes/gitignore)\n",
    "\n",
    "\n",
    "Not covered here but relevant to your upcoming assignment:\n",
    "\n",
    "- [Spotify API documentation](https://developer.spotify.com/documentation/web-api/)\n",
    "- [Spotify Getting Started Guide](https://developer.spotify.com/documentation/web-api/quick-start/)\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**‚öôÔ∏è SETUP**\n",
    "\n",
    "Before you continue, set up your Python environment. Check the instructions under the ['üêç Python environment' section on README](../README.md#üêç-python-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read the JSON files\n",
    "\n",
    "At the time of writing this notebook, we have collected the top 1000 posts from the subreddit `r/datascience` and saved them in a single JSON file. \n",
    "\n",
    "Our goal now is to store this data in a database. But before we do that, let's read the JSON file and inspect its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KEARNEY2\\Dropbox\\Github\\DS105a_v2\\ds105a-2024\\W08\\code\n"
     ]
    }
   ],
   "source": [
    "# get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "kind",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "data",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "bb1bf82e-435f-4b81-81ed-35d58fc0314d",
       "rows": [
        [
         "0",
         "t3",
         "{'approved_at_utc': None, 'subreddit': 'datascience', 'selftext': '', 'author_fullname': 't2_sejrw', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How to avoid 1/2-assed data analysis', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/datascience', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1f6ztk4', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.98, 'author_flair_background_color': None, 'ups': 3122, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Monday Meme', 'can_mod_post': False, 'score': 3122, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/-IRbYaOpgZ3d5MOSS_Om2syoauQKMsUzySyDTeqsxQE.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1725257884.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': 'confidence', 'banned_at_utc': None, 'url_overridden_by_dest': 'https://i.redd.it/l6zd3zsa9cmd1.jpeg', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': True, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/l6zd3zsa9cmd1.jpeg?auto=webp&amp;s=d09b1c4322a01edc9e4255ffb102dd176ebc58c3', 'width': 500, 'height': 571}, 'resolutions': [{'url': 'https://preview.redd.it/l6zd3zsa9cmd1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=70d72eafb0912a132f122bc3c3feab15919f9657', 'width': 108, 'height': 123}, {'url': 'https://preview.redd.it/l6zd3zsa9cmd1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b2c34bb25b9601e1070bd90fb7a7c3be71459e08', 'width': 216, 'height': 246}, {'url': 'https://preview.redd.it/l6zd3zsa9cmd1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=31f842ccc50affb3f287dd7f55e3a1fa4631f561', 'width': 320, 'height': 365}], 'variants': {}, 'id': '6HsUoGWv3M6hraJsntR-G1zJJ_5guFL8po4YyLPbl0g'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '6e90f572-70ec-11ee-9bd6-2692ba006635', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_2sptq', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ff8717', 'id': '1f6ztk4', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'BdR76', 'discussion_type': None, 'num_comments': 56, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/datascience/comments/1f6ztk4/how_to_avoid_12assed_data_analysis/', 'stickied': False, 'url': 'https://i.redd.it/l6zd3zsa9cmd1.jpeg', 'subreddit_subscribers': 2366517, 'created_utc': 1725257884.0, 'num_crossposts': 2, 'media': None, 'is_video': False}"
        ],
        [
         "1",
         "t3",
         "{'approved_at_utc': None, 'subreddit': 'datascience', 'selftext': \"What's your favourite one line code.\", 'author_fullname': 't2_dh9o0mnfv', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Favourite piece of code ü§£', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/datascience', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'discussion', 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1feyx6h', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.98, 'author_flair_background_color': None, 'ups': 2810, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2810, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/TfxvD7zmWAB1n7mP9lXDqlllEnhFR92WwsiLR54xbsY.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1726135600.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s your favourite one line code.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': 'confidence', 'banned_at_utc': None, 'url_overridden_by_dest': 'https://i.redd.it/jqvbe1ycrcod1.jpeg', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': True, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/jqvbe1ycrcod1.jpeg?auto=webp&amp;s=df5f8ef4b3708486adbf911e9bfd791b091b852b', 'width': 2160, 'height': 3840}, 'resolutions': [{'url': 'https://preview.redd.it/jqvbe1ycrcod1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2327e716cc1afd534ba0e972f060c347ca562134', 'width': 108, 'height': 192}, {'url': 'https://preview.redd.it/jqvbe1ycrcod1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=039e51219757e8a089808dfe8d5fba74f814334e', 'width': 216, 'height': 384}, {'url': 'https://preview.redd.it/jqvbe1ycrcod1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a575ab05712d202770ba51d580369bea32682272', 'width': 320, 'height': 568}, {'url': 'https://preview.redd.it/jqvbe1ycrcod1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ae5d03f3d1b8e77146cd2a0e42623e3402b54410', 'width': 640, 'height': 1137}, {'url': 'https://preview.redd.it/jqvbe1ycrcod1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=968170170d0ae6a61004a8d06b907d3ab46efb11', 'width': 960, 'height': 1706}, {'url': 'https://preview.redd.it/jqvbe1ycrcod1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d608dd26c7dc3a153b90897c08d76ef4c218a58a', 'width': 1080, 'height': 1920}], 'variants': {}, 'id': 'qSGb6VZVkUE3TbKzvqJ_Nq5DrOLKbcVJlp3zdzuC0RA'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '4fad7108-d77d-11e7-b0c6-0ee69f155af2', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_2sptq', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#1a1a1b', 'id': '1feyx6h', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'nobody_undefined', 'discussion_type': None, 'num_comments': 102, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/datascience/comments/1feyx6h/favourite_piece_of_code/', 'stickied': False, 'url': 'https://i.redd.it/jqvbe1ycrcod1.jpeg', 'subreddit_subscribers': 2366517, 'created_utc': 1726135600.0, 'num_crossposts': 2, 'media': None, 'is_video': False}"
        ],
        [
         "2",
         "t3",
         "{'approved_at_utc': None, 'subreddit': 'datascience', 'selftext': '', 'author_fullname': 't2_sejrw', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': \"You're not helping, Excel! please STOP HELPING!!!\", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/datascience', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 106, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1dsnbww', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.97, 'author_flair_background_color': None, 'ups': 1808, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': True, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Monday Meme', 'can_mod_post': False, 'score': 1808, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/DRZigIG8rRJG6q6_6ibDlZ9MimSIyTJE6Cz_qrbVCDU.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'image', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1719819987.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'i.redd.it', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': 'confidence', 'banned_at_utc': None, 'url_overridden_by_dest': 'https://i.redd.it/ccxo0ajs3v9d1.jpeg', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': True, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://preview.redd.it/ccxo0ajs3v9d1.jpeg?auto=webp&amp;s=867aab568a2d3f4696a43340291925dece3f7c2e', 'width': 573, 'height': 435}, 'resolutions': [{'url': 'https://preview.redd.it/ccxo0ajs3v9d1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a5cbb2d90d543837892d9d424bd01594d40d8b75', 'width': 108, 'height': 81}, {'url': 'https://preview.redd.it/ccxo0ajs3v9d1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d4f4cd3bade370a96b8155e63c42e6deb83425b9', 'width': 216, 'height': 163}, {'url': 'https://preview.redd.it/ccxo0ajs3v9d1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3b9e7c1622287ec2e4482d057a0a4e62effa4be5', 'width': 320, 'height': 242}], 'variants': {}, 'id': 'bvCCv9qNFinbHfDv30IsNCSyLb-Xj4BiT-0BlGQM0kg'}], 'enabled': True}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '6e90f572-70ec-11ee-9bd6-2692ba006635', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_2sptq', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ff8717', 'id': '1dsnbww', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'BdR76', 'discussion_type': None, 'num_comments': 155, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/datascience/comments/1dsnbww/youre_not_helping_excel_please_stop_helping/', 'stickied': False, 'url': 'https://i.redd.it/ccxo0ajs3v9d1.jpeg', 'subreddit_subscribers': 2366517, 'created_utc': 1719819987.0, 'num_crossposts': 1, 'media': None, 'is_video': False}"
        ],
        [
         "3",
         "t3",
         "{'approved_at_utc': None, 'subreddit': 'datascience', 'selftext': \"The year just started and there are already over 50K layoffs. The latest one is UPS, including some data professionals at corporate. These are people who worked hard, built a career with the company over extremely long period of time, stayed loyal, 3% merit increases, worked extra hours because they believed that they were contributing to a better future for the company and themselves.... And they were laid off without a second thought for cost saving. Yeah, Because that makes so much sense, right? Record-breaking profits every year is an unattainable goal, and it's stupid that here in the USA, we are one of the only countries that keeps pushing for this while other countries are leaving us in the dust with their quality of life....\\n\\n\\nSo just remember. If you're thinking about doing some overtime for free, or going above and beyond just for a pat on the back, don't do it. You only have so many years on Earth. Focus on your own life and prioritize yourself, always\", 'user_reports': [], 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': \"Friendly reminder not to work too hard. You'll just get fired\", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/datascience', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'discussion', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1afgvv9', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.93, 'author_flair_background_color': '', 'subreddit_type': 'public', 'ups': 1704, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 1704, 'approved_by': None, 'is_created_from_ads_ui': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1706704645.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'domain': 'self.datascience', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The year just started and there are already over 50K layoffs. The latest one is UPS, including some data professionals at corporate. These are people who worked hard, built a career with the company over extremely long period of time, stayed loyal, 3% merit increases, worked extra hours because they believed that they were contributing to a better future for the company and themselves.... And they were laid off without a second thought for cost saving. Yeah, Because that makes so much sense, right? Record-breaking profits every year is an unattainable goal, and it&amp;#39;s stupid that here in the USA, we are one of the only countries that keeps pushing for this while other countries are leaving us in the dust with their quality of life....&lt;/p&gt;\\n\\n&lt;p&gt;So just remember. If you&amp;#39;re thinking about doing some overtime for free, or going above and beyond just for a pat on the back, don&amp;#39;t do it. You only have so many years on Earth. Focus on your own life and prioritize yourself, always&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': 'confidence', 'banned_at_utc': None, 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': True, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '4fad7108-d77d-11e7-b0c6-0ee69f155af2', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2sptq', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#1a1a1b', 'id': '1afgvv9', 'is_robot_indexable': True, 'report_reasons': None, 'author': '[deleted]', 'discussion_type': None, 'num_comments': 204, 'send_replies': False, 'contest_mode': False, 'mod_reports': [], 'author_flair_text_color': 'dark', 'permalink': '/r/datascience/comments/1afgvv9/friendly_reminder_not_to_work_too_hard_youll_just/', 'stickied': False, 'url': 'https://www.reddit.com/r/datascience/comments/1afgvv9/friendly_reminder_not_to_work_too_hard_youll_just/', 'subreddit_subscribers': 2366517, 'created_utc': 1706704645.0, 'num_crossposts': 0, 'media': None, 'is_video': False}"
        ],
        [
         "4",
         "t3",
         "{'approved_at_utc': None, 'subreddit': 'datascience', 'selftext': 'In summary and basically talks about how she was managing a high priority product at Spotify after 3 years at Spotify. She was the ONLY DATA SCIENTIST working on this project and with pushy stakeholders she was working 14-15 hour days. Frankly this would piss me the fuck off. How the hell does some shit like this even happen? How common is this? For a place like Spotify it sounds quite shocking. How do you manage a ‚Äúpushy‚Äù stakeholder?', 'author_fullname': 't2_uy28jztl', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data scientist quits her job at Spotify', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/datascience', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'discussion', 'downs': 0, 'thumbnail_height': 105, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1b1au2f', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.91, 'author_flair_background_color': None, 'ups': 1398, 'total_awards_received': 0, 'media_embed': {'content': '&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OMI4Wu9wnY0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"I Quit My Job as a Data Scientist at Spotify\"&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'height': 200}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': {'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'I Quit My Job as a Data Scientist at Spotify', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OMI4Wu9wnY0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"I Quit My Job as a Data Scientist at Spotify\"&gt;&lt;/iframe&gt;', 'author_name': 'julia fei', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/OMI4Wu9wnY0/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/@juliafei'}}, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {'content': '&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OMI4Wu9wnY0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"I Quit My Job as a Data Scientist at Spotify\"&gt;&lt;/iframe&gt;', 'width': 356, 'scrolling': False, 'media_domain_url': 'https://www.redditmedia.com/mediaembed/1b1au2f', 'height': 200}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 1398, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/o2CZSMr4qu0_VYR8l0LZrxyy6zHwlLL9qvmpE0k_lfo.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'rich:video', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1709037427.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'youtu.be', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In summary and basically talks about how she was managing a high priority product at Spotify after 3 years at Spotify. She was the ONLY DATA SCIENTIST working on this project and with pushy stakeholders she was working 14-15 hour days. Frankly this would piss me the fuck off. How the hell does some shit like this even happen? How common is this? For a place like Spotify it sounds quite shocking. How do you manage a ‚Äúpushy‚Äù stakeholder?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': 'confidence', 'banned_at_utc': None, 'url_overridden_by_dest': 'https://youtu.be/OMI4Wu9wnY0?si=teFkXgTnPmUAuAyU', 'view_count': None, 'archived': True, 'no_follow': False, 'is_crosspostable': True, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/YkDmw7N33PvUpJhUIPyxUJ_4x4q1yJrqFQUX4e4uNwE.jpg?auto=webp&amp;s=5cbbb3c4a8eb35cf900d4ad57578f9c8bf5d0c62', 'width': 480, 'height': 360}, 'resolutions': [{'url': 'https://external-preview.redd.it/YkDmw7N33PvUpJhUIPyxUJ_4x4q1yJrqFQUX4e4uNwE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a8f1c2c04e81a4a8ffbec32b6c9997649255e556', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/YkDmw7N33PvUpJhUIPyxUJ_4x4q1yJrqFQUX4e4uNwE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=181cae332381e605475c75e786d41913f386db00', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/YkDmw7N33PvUpJhUIPyxUJ_4x4q1yJrqFQUX4e4uNwE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bab0884e7eb0768b1c6d60f80580238078e13e32', 'width': 320, 'height': 240}], 'variants': {}, 'id': 'uI3kvCIwEXFXwWwDnNExiA3G_he8S2CimbSl84Zo-p4'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '4fad7108-d77d-11e7-b0c6-0ee69f155af2', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_2sptq', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#1a1a1b', 'id': '1b1au2f', 'is_robot_indexable': True, 'report_reasons': None, 'author': 'Direct-Touch469', 'discussion_type': None, 'num_comments': 373, 'send_replies': True, 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/datascience/comments/1b1au2f/data_scientist_quits_her_job_at_spotify/', 'stickied': False, 'url': 'https://youtu.be/OMI4Wu9wnY0?si=teFkXgTnPmUAuAyU', 'subreddit_subscribers': 2366517, 'created_utc': 1709037427.0, 'num_crossposts': 1, 'media': {'type': 'youtube.com', 'oembed': {'provider_url': 'https://www.youtube.com/', 'version': '1.0', 'title': 'I Quit My Job as a Data Scientist at Spotify', 'type': 'video', 'thumbnail_width': 480, 'height': 200, 'width': 356, 'html': '&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/OMI4Wu9wnY0?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"I Quit My Job as a Data Scientist at Spotify\"&gt;&lt;/iframe&gt;', 'author_name': 'julia fei', 'provider_name': 'YouTube', 'thumbnail_url': 'https://i.ytimg.com/vi/OMI4Wu9wnY0/hqdefault.jpg', 'thumbnail_height': 360, 'author_url': 'https://www.youtube.com/@juliafei'}}, 'is_video': False}"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kind</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'datasc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'datasc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'datasc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'datasc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'datasc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  kind                                               data\n",
       "0   t3  {'approved_at_utc': None, 'subreddit': 'datasc...\n",
       "1   t3  {'approved_at_utc': None, 'subreddit': 'datasc...\n",
       "2   t3  {'approved_at_utc': None, 'subreddit': 'datasc...\n",
       "3   t3  {'approved_at_utc': None, 'subreddit': 'datasc...\n",
       "4   t3  {'approved_at_utc': None, 'subreddit': 'datasc..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = '../data/reddit/raw/datascience_top1000_2024_11_21_09_56.json'\n",
    "\n",
    "# How does the data look like?\n",
    "pd.read_json(filepath).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using my knowledge of Week 07, I know that there is a `data` key in the JSON file that contains a list of dictionaries. Each dictionary represents a post in the subreddit. I can use the `pd.json_normalize()` function to flatten this list of dictionaries into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts: 1000 (üéâ AS EXPECTED)\n"
     ]
    }
   ],
   "source": [
    "df = pd.json_normalize(pd.read_json(filepath)['data'])\n",
    "\n",
    "print(f\"Number of posts: {df.shape[0]} {'(üéâ AS EXPECTED)' if df.shape[0] == 1000 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Inspecting the DataFrame (to study later)\n",
    "\n",
    "In this SECTION, I demonstrate how I would inspect this unfamiliar DataFrame to understand its structure and decide which columns to keep.\n",
    "\n",
    "First of all, my interest with the current endpoint lies in understanding the type of content that is heavily favoured in the `r/datascience` subreddit. I might also want to look at, say, the comments later but for now, I will focus on the posts themselves. Let's check what each of these columns represent and decide which ones we want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Columns: 870 entries, approved_at_utc to media_metadata.8u50wr1mo7ic1.id\n",
      "dtypes: bool(28), float64(199), int64(10), object(633)\n",
      "memory usage: 6.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# Oh no, there are 870 columns!\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['approved_at_utc', 'subreddit', 'selftext', 'author_fullname', 'saved',\n",
       "       'mod_reason_title', 'gilded', 'clicked', 'title', 'link_flair_richtext',\n",
       "       ...\n",
       "       'media.oembed.description', 'media.oembed.mean_alpha',\n",
       "       'media_metadata.8u50wr1mo7ic1.status', 'media_metadata.8u50wr1mo7ic1.e',\n",
       "       'media_metadata.8u50wr1mo7ic1.m', 'media_metadata.8u50wr1mo7ic1.p',\n",
       "       'media_metadata.8u50wr1mo7ic1.s.y', 'media_metadata.8u50wr1mo7ic1.s.x',\n",
       "       'media_metadata.8u50wr1mo7ic1.s.u', 'media_metadata.8u50wr1mo7ic1.id'],\n",
       "      dtype='object', length=870)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are just too many!\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate columns that were automatically normalised\n",
    "\n",
    "All the columns that have a `.` in their name were automatically normalised by `pd.json_normalize()`. \n",
    "\n",
    "Some had deeper nested structures, producing more `.` in their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 762 columns with `.` in their name.\n"
     ]
    }
   ],
   "source": [
    "# Loop over all column names and identify those with `.` in them\n",
    "\n",
    "normalized_cols = [col for col in df.columns if \".\" in col]\n",
    "print(f\"There are {len(normalized_cols)} columns with `.` in their name.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, so A LOT of columns start with `data.`. Let's rename them to make it easier to work with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove columns by prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the prevalent prefixes in the column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "479f63f1-d391-4331-b79a-418bac9150c1",
       "rows": [
        [
         "media_metadata",
         "718"
        ],
        [
         "secure_media",
         "16"
        ],
        [
         "media",
         "16"
        ],
        [
         "secure_media_embed",
         "5"
        ],
        [
         "media_embed",
         "4"
        ],
        [
         "preview",
         "2"
        ],
        [
         "gallery_data",
         "1"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 7
       }
      },
      "text/plain": [
       "media_metadata        718\n",
       "secure_media           16\n",
       "media                  16\n",
       "secure_media_embed      5\n",
       "media_embed             4\n",
       "preview                 2\n",
       "gallery_data            1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefixes = pd.Series([col.split(\".\")[0] for col in normalized_cols if \".\" in col]).value_counts()\n",
    "prefixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't think I care about any of those! Let's drop them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['media_metadata', 'secure_media', 'media', 'secure_media_embed',\n",
       "       'media_embed', 'preview', 'gallery_data'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefixes.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Now here's a pro-tip!** If you want to check if a string starts with a prefix you can do:\n",
    "\n",
    "```python\n",
    "my_string_variable.startswith('prefix')\n",
    "```\n",
    "\n",
    "To check if the string contains any of the prefixes in a list, you can do:\n",
    "\n",
    "```python\n",
    "any([my_string_variable.startswith(prefix) for prefix in my_prefixes_list])\n",
    "```\n",
    "\n",
    "But there is a nicer way to do that! You can simply pass a [tuple](https://www.w3schools.com/python/python_tuples.asp) of prefixes to the `str.startswith()` method:\n",
    "\n",
    "```python\n",
    "my_string_variable.startswith(('prefix1', 'prefix2', 'prefix3'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_prefixes = ('secure_media', 'media', 'preview', 'gallery_data')\n",
    "columns_to_drop = [col for col in df.columns if col.startswith(irrelevant_prefixes)]\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['secure_media', 'media_only', 'media', 'preview.images', 'preview.enabled', 'media_embed.content', 'media_embed.width', 'media_embed.scrolling', 'media_embed.height', 'secure_media.type', 'secure_media.oembed.provider_url', 'secure_media.oembed.version', 'secure_media.oembed.title', 'secure_media.oembed.type', 'secure_media.oembed.thumbnail_width', 'secure_media.oembed.height', 'secure_media.oembed.width', 'secure_media.oembed.html', 'secure_media.oembed.author_name', 'secure_media.oembed.provider_name', 'secure_media.oembed.thumbnail_url', 'secure_media.oembed.thumbnail_height', 'secure_media.oembed.author_url', 'secure_media_embed.content', 'secure_media_embed.width', 'secure_media_embed.scrolling', 'secure_media_embed.media_domain_url', 'secure_media_embed.height', 'media.type', 'media.oembed.provider_url', 'media.oembed.version', 'media.oembed.title', 'media.oembed.type', 'media.oembed.thumbnail_width', 'media.oembed.height', 'media.oembed.width', 'media.oembed.html', 'media.oembed.author_name', 'media.oembed.provider_name', 'media.oembed.thumbnail_url', 'media.oembed.thumbnail_height', 'media.oembed.author_url', 'media_metadata.669eo83ac30d1.status', 'media_metadata.669eo83ac30d1.e', 'media_metadata.669eo83ac30d1.m', 'media_metadata.669eo83ac30d1.p', 'media_metadata.669eo83ac30d1.s.y', 'media_metadata.669eo83ac30d1.s.x', 'media_metadata.669eo83ac30d1.s.u', 'media_metadata.669eo83ac30d1.id', 'media_metadata.xxrkz93ac30d1.status', 'media_metadata.xxrkz93ac30d1.e', 'media_metadata.xxrkz93ac30d1.m', 'media_metadata.xxrkz93ac30d1.p', 'media_metadata.xxrkz93ac30d1.s.y', 'media_metadata.xxrkz93ac30d1.s.x', 'media_metadata.xxrkz93ac30d1.s.u', 'media_metadata.xxrkz93ac30d1.id', 'gallery_data.items', 'media_metadata.b89q4likmupc1.status', 'media_metadata.b89q4likmupc1.e', 'media_metadata.b89q4likmupc1.m', 'media_metadata.b89q4likmupc1.p', 'media_metadata.b89q4likmupc1.s.y', 'media_metadata.b89q4likmupc1.s.x', 'media_metadata.b89q4likmupc1.s.u', 'media_metadata.b89q4likmupc1.id', 'media_metadata.oy2d2asaofsc1.status', 'media_metadata.oy2d2asaofsc1.e', 'media_metadata.oy2d2asaofsc1.m', 'media_metadata.oy2d2asaofsc1.p', 'media_metadata.oy2d2asaofsc1.s.y', 'media_metadata.oy2d2asaofsc1.s.x', 'media_metadata.oy2d2asaofsc1.s.u', 'media_metadata.oy2d2asaofsc1.id', 'media_metadata.s8po7asaofsc1.status', 'media_metadata.s8po7asaofsc1.e', 'media_metadata.s8po7asaofsc1.m', 'media_metadata.s8po7asaofsc1.p', 'media_metadata.s8po7asaofsc1.s.y', 'media_metadata.s8po7asaofsc1.s.x', 'media_metadata.s8po7asaofsc1.s.u', 'media_metadata.s8po7asaofsc1.id', 'media_metadata.eutfjzpn72ec1.status', 'media_metadata.eutfjzpn72ec1.e', 'media_metadata.eutfjzpn72ec1.m', 'media_metadata.eutfjzpn72ec1.p', 'media_metadata.eutfjzpn72ec1.s.y', 'media_metadata.eutfjzpn72ec1.s.x', 'media_metadata.eutfjzpn72ec1.s.u', 'media_metadata.eutfjzpn72ec1.id', 'media_metadata.9hbqoypn72ec1.status', 'media_metadata.9hbqoypn72ec1.e', 'media_metadata.9hbqoypn72ec1.m', 'media_metadata.9hbqoypn72ec1.p', 'media_metadata.9hbqoypn72ec1.s.y', 'media_metadata.9hbqoypn72ec1.s.x', 'media_metadata.9hbqoypn72ec1.s.u', 'media_metadata.9hbqoypn72ec1.id', 'media_metadata.37pi87paz4td1.status', 'media_metadata.37pi87paz4td1.e', 'media_metadata.37pi87paz4td1.m', 'media_metadata.37pi87paz4td1.p', 'media_metadata.37pi87paz4td1.s.y', 'media_metadata.37pi87paz4td1.s.x', 'media_metadata.37pi87paz4td1.s.u', 'media_metadata.37pi87paz4td1.id', 'media_metadata.qc0id6paz4td1.status', 'media_metadata.qc0id6paz4td1.e', 'media_metadata.qc0id6paz4td1.m', 'media_metadata.qc0id6paz4td1.p', 'media_metadata.qc0id6paz4td1.s.y', 'media_metadata.qc0id6paz4td1.s.x', 'media_metadata.qc0id6paz4td1.s.u', 'media_metadata.qc0id6paz4td1.id', 'media_metadata.ipe567paz4td1.status', 'media_metadata.ipe567paz4td1.e', 'media_metadata.ipe567paz4td1.m', 'media_metadata.ipe567paz4td1.p', 'media_metadata.ipe567paz4td1.s.y', 'media_metadata.ipe567paz4td1.s.x', 'media_metadata.ipe567paz4td1.s.u', 'media_metadata.ipe567paz4td1.id', 'media_metadata.oixj0kbe306d1.status', 'media_metadata.oixj0kbe306d1.e', 'media_metadata.oixj0kbe306d1.m', 'media_metadata.oixj0kbe306d1.p', 'media_metadata.oixj0kbe306d1.s.y', 'media_metadata.oixj0kbe306d1.s.gif', 'media_metadata.oixj0kbe306d1.s.mp4', 'media_metadata.oixj0kbe306d1.s.x', 'media_metadata.oixj0kbe306d1.id', 'media_metadata.upe6uhebzz5d1.status', 'media_metadata.upe6uhebzz5d1.e', 'media_metadata.upe6uhebzz5d1.m', 'media_metadata.upe6uhebzz5d1.p', 'media_metadata.upe6uhebzz5d1.s.y', 'media_metadata.upe6uhebzz5d1.s.gif', 'media_metadata.upe6uhebzz5d1.s.mp4', 'media_metadata.upe6uhebzz5d1.s.x', 'media_metadata.upe6uhebzz5d1.id', 'media_metadata.ovcp8hcgb1qc1.status', 'media_metadata.ovcp8hcgb1qc1.e', 'media_metadata.ovcp8hcgb1qc1.m', 'media_metadata.ovcp8hcgb1qc1.p', 'media_metadata.ovcp8hcgb1qc1.s.y', 'media_metadata.ovcp8hcgb1qc1.s.x', 'media_metadata.ovcp8hcgb1qc1.s.u', 'media_metadata.ovcp8hcgb1qc1.id', 'media_metadata.8giy5tr8i1qc1.status', 'media_metadata.8giy5tr8i1qc1.e', 'media_metadata.8giy5tr8i1qc1.m', 'media_metadata.8giy5tr8i1qc1.p', 'media_metadata.8giy5tr8i1qc1.s.y', 'media_metadata.8giy5tr8i1qc1.s.x', 'media_metadata.8giy5tr8i1qc1.s.u', 'media_metadata.8giy5tr8i1qc1.id', 'media_metadata.9uxuobuti1qc1.status', 'media_metadata.9uxuobuti1qc1.e', 'media_metadata.9uxuobuti1qc1.m', 'media_metadata.9uxuobuti1qc1.p', 'media_metadata.9uxuobuti1qc1.s.y', 'media_metadata.9uxuobuti1qc1.s.x', 'media_metadata.9uxuobuti1qc1.s.u', 'media_metadata.9uxuobuti1qc1.id', 'media_metadata.pimy1i38a1qc1.status', 'media_metadata.pimy1i38a1qc1.e', 'media_metadata.pimy1i38a1qc1.m', 'media_metadata.pimy1i38a1qc1.p', 'media_metadata.pimy1i38a1qc1.s.y', 'media_metadata.pimy1i38a1qc1.s.x', 'media_metadata.pimy1i38a1qc1.s.u', 'media_metadata.pimy1i38a1qc1.id', 'media_metadata.odavl54yh1qc1.status', 'media_metadata.odavl54yh1qc1.e', 'media_metadata.odavl54yh1qc1.m', 'media_metadata.odavl54yh1qc1.p', 'media_metadata.odavl54yh1qc1.s.y', 'media_metadata.odavl54yh1qc1.s.x', 'media_metadata.odavl54yh1qc1.s.u', 'media_metadata.odavl54yh1qc1.id', 'media_metadata.3rrwu8rfa1qc1.status', 'media_metadata.3rrwu8rfa1qc1.e', 'media_metadata.3rrwu8rfa1qc1.m', 'media_metadata.3rrwu8rfa1qc1.p', 'media_metadata.3rrwu8rfa1qc1.s.y', 'media_metadata.3rrwu8rfa1qc1.s.x', 'media_metadata.3rrwu8rfa1qc1.s.u', 'media_metadata.3rrwu8rfa1qc1.id', 'media_metadata.fsu1yqg0j1qc1.status', 'media_metadata.fsu1yqg0j1qc1.e', 'media_metadata.fsu1yqg0j1qc1.m', 'media_metadata.fsu1yqg0j1qc1.p', 'media_metadata.fsu1yqg0j1qc1.s.y', 'media_metadata.fsu1yqg0j1qc1.s.x', 'media_metadata.fsu1yqg0j1qc1.s.u', 'media_metadata.fsu1yqg0j1qc1.id', 'media_metadata.2ljyfl9aj1qc1.status', 'media_metadata.2ljyfl9aj1qc1.e', 'media_metadata.2ljyfl9aj1qc1.m', 'media_metadata.2ljyfl9aj1qc1.p', 'media_metadata.2ljyfl9aj1qc1.s.y', 'media_metadata.2ljyfl9aj1qc1.s.x', 'media_metadata.2ljyfl9aj1qc1.s.u', 'media_metadata.2ljyfl9aj1qc1.id', 'media_metadata.5dcgo35jj1qc1.status', 'media_metadata.5dcgo35jj1qc1.e', 'media_metadata.5dcgo35jj1qc1.m', 'media_metadata.5dcgo35jj1qc1.p', 'media_metadata.5dcgo35jj1qc1.s.y', 'media_metadata.5dcgo35jj1qc1.s.x', 'media_metadata.5dcgo35jj1qc1.s.u', 'media_metadata.5dcgo35jj1qc1.id', 'media_metadata.501h2r27b1qc1.status', 'media_metadata.501h2r27b1qc1.e', 'media_metadata.501h2r27b1qc1.m', 'media_metadata.501h2r27b1qc1.p', 'media_metadata.501h2r27b1qc1.s.y', 'media_metadata.501h2r27b1qc1.s.x', 'media_metadata.501h2r27b1qc1.s.u', 'media_metadata.501h2r27b1qc1.id', 'media_metadata.tfbcdin8f1qc1.status', 'media_metadata.tfbcdin8f1qc1.e', 'media_metadata.tfbcdin8f1qc1.m', 'media_metadata.tfbcdin8f1qc1.p', 'media_metadata.tfbcdin8f1qc1.s.y', 'media_metadata.tfbcdin8f1qc1.s.x', 'media_metadata.tfbcdin8f1qc1.s.u', 'media_metadata.tfbcdin8f1qc1.id', 'media_metadata.j2m2vqrh81gc1.status', 'media_metadata.j2m2vqrh81gc1.e', 'media_metadata.j2m2vqrh81gc1.m', 'media_metadata.j2m2vqrh81gc1.p', 'media_metadata.j2m2vqrh81gc1.s.y', 'media_metadata.j2m2vqrh81gc1.s.gif', 'media_metadata.j2m2vqrh81gc1.s.mp4', 'media_metadata.j2m2vqrh81gc1.s.x', 'media_metadata.j2m2vqrh81gc1.id', 'media_metadata.jmh961ge81gc1.status', 'media_metadata.jmh961ge81gc1.e', 'media_metadata.jmh961ge81gc1.m', 'media_metadata.jmh961ge81gc1.p', 'media_metadata.jmh961ge81gc1.s.y', 'media_metadata.jmh961ge81gc1.s.gif', 'media_metadata.jmh961ge81gc1.s.mp4', 'media_metadata.jmh961ge81gc1.s.x', 'media_metadata.jmh961ge81gc1.id', 'media_metadata.1zivd85a81gc1.status', 'media_metadata.1zivd85a81gc1.e', 'media_metadata.1zivd85a81gc1.m', 'media_metadata.1zivd85a81gc1.p', 'media_metadata.1zivd85a81gc1.s.y', 'media_metadata.1zivd85a81gc1.s.gif', 'media_metadata.1zivd85a81gc1.s.mp4', 'media_metadata.1zivd85a81gc1.s.x', 'media_metadata.1zivd85a81gc1.id', 'media_metadata.jy1k45h13dbd1.status', 'media_metadata.jy1k45h13dbd1.e', 'media_metadata.jy1k45h13dbd1.m', 'media_metadata.jy1k45h13dbd1.p', 'media_metadata.jy1k45h13dbd1.s.y', 'media_metadata.jy1k45h13dbd1.s.x', 'media_metadata.jy1k45h13dbd1.s.u', 'media_metadata.jy1k45h13dbd1.id', 'media_metadata.waoiwcz03dbd1.status', 'media_metadata.waoiwcz03dbd1.e', 'media_metadata.waoiwcz03dbd1.m', 'media_metadata.waoiwcz03dbd1.p', 'media_metadata.waoiwcz03dbd1.s.y', 'media_metadata.waoiwcz03dbd1.s.x', 'media_metadata.waoiwcz03dbd1.s.u', 'media_metadata.waoiwcz03dbd1.id', 'media_metadata.bynyc4kj3rud1.status', 'media_metadata.bynyc4kj3rud1.e', 'media_metadata.bynyc4kj3rud1.m', 'media_metadata.bynyc4kj3rud1.p', 'media_metadata.bynyc4kj3rud1.s.y', 'media_metadata.bynyc4kj3rud1.s.x', 'media_metadata.bynyc4kj3rud1.s.u', 'media_metadata.bynyc4kj3rud1.id', 'media_metadata.ravz54z7d49d1.status', 'media_metadata.ravz54z7d49d1.e', 'media_metadata.ravz54z7d49d1.m', 'media_metadata.ravz54z7d49d1.p', 'media_metadata.ravz54z7d49d1.s.y', 'media_metadata.ravz54z7d49d1.s.x', 'media_metadata.ravz54z7d49d1.s.u', 'media_metadata.ravz54z7d49d1.id', 'media_metadata.7hd2u0n7plkd1.status', 'media_metadata.7hd2u0n7plkd1.e', 'media_metadata.7hd2u0n7plkd1.m', 'media_metadata.7hd2u0n7plkd1.p', 'media_metadata.7hd2u0n7plkd1.s.y', 'media_metadata.7hd2u0n7plkd1.s.x', 'media_metadata.7hd2u0n7plkd1.s.u', 'media_metadata.7hd2u0n7plkd1.id', 'media_metadata.b6jet0n7plkd1.status', 'media_metadata.b6jet0n7plkd1.e', 'media_metadata.b6jet0n7plkd1.m', 'media_metadata.b6jet0n7plkd1.p', 'media_metadata.b6jet0n7plkd1.s.y', 'media_metadata.b6jet0n7plkd1.s.x', 'media_metadata.b6jet0n7plkd1.s.u', 'media_metadata.b6jet0n7plkd1.id', 'media_metadata.y0fmapr2u2ed1.status', 'media_metadata.y0fmapr2u2ed1.e', 'media_metadata.y0fmapr2u2ed1.m', 'media_metadata.y0fmapr2u2ed1.p', 'media_metadata.y0fmapr2u2ed1.s.y', 'media_metadata.y0fmapr2u2ed1.s.x', 'media_metadata.y0fmapr2u2ed1.s.u', 'media_metadata.y0fmapr2u2ed1.id', 'media_metadata.5ou0awyehigc1.status', 'media_metadata.5ou0awyehigc1.e', 'media_metadata.5ou0awyehigc1.m', 'media_metadata.5ou0awyehigc1.p', 'media_metadata.5ou0awyehigc1.s.y', 'media_metadata.5ou0awyehigc1.s.x', 'media_metadata.5ou0awyehigc1.s.u', 'media_metadata.5ou0awyehigc1.id', 'media_metadata.jl3cyhojhigc1.status', 'media_metadata.jl3cyhojhigc1.e', 'media_metadata.jl3cyhojhigc1.m', 'media_metadata.jl3cyhojhigc1.p', 'media_metadata.jl3cyhojhigc1.s.y', 'media_metadata.jl3cyhojhigc1.s.x', 'media_metadata.jl3cyhojhigc1.s.u', 'media_metadata.jl3cyhojhigc1.id', 'media_metadata.kk0mrflphigc1.status', 'media_metadata.kk0mrflphigc1.e', 'media_metadata.kk0mrflphigc1.m', 'media_metadata.kk0mrflphigc1.p', 'media_metadata.kk0mrflphigc1.s.y', 'media_metadata.kk0mrflphigc1.s.x', 'media_metadata.kk0mrflphigc1.s.u', 'media_metadata.kk0mrflphigc1.id', 'media_metadata.j6l02trmhigc1.status', 'media_metadata.j6l02trmhigc1.e', 'media_metadata.j6l02trmhigc1.m', 'media_metadata.j6l02trmhigc1.p', 'media_metadata.j6l02trmhigc1.s.y', 'media_metadata.j6l02trmhigc1.s.x', 'media_metadata.j6l02trmhigc1.s.u', 'media_metadata.j6l02trmhigc1.id', 'media_metadata.pmkgqpuwhigc1.status', 'media_metadata.pmkgqpuwhigc1.e', 'media_metadata.pmkgqpuwhigc1.m', 'media_metadata.pmkgqpuwhigc1.p', 'media_metadata.pmkgqpuwhigc1.s.y', 'media_metadata.pmkgqpuwhigc1.s.x', 'media_metadata.pmkgqpuwhigc1.s.u', 'media_metadata.pmkgqpuwhigc1.id', 'media_metadata.a0nuru70iigc1.status', 'media_metadata.a0nuru70iigc1.e', 'media_metadata.a0nuru70iigc1.m', 'media_metadata.a0nuru70iigc1.p', 'media_metadata.a0nuru70iigc1.s.y', 'media_metadata.a0nuru70iigc1.s.x', 'media_metadata.a0nuru70iigc1.s.u', 'media_metadata.a0nuru70iigc1.id', 'media_metadata.dah6fe3kgigc1.status', 'media_metadata.dah6fe3kgigc1.e', 'media_metadata.dah6fe3kgigc1.m', 'media_metadata.dah6fe3kgigc1.p', 'media_metadata.dah6fe3kgigc1.s.y', 'media_metadata.dah6fe3kgigc1.s.x', 'media_metadata.dah6fe3kgigc1.s.u', 'media_metadata.dah6fe3kgigc1.id', 'media_metadata.s7zorqm9higc1.status', 'media_metadata.s7zorqm9higc1.e', 'media_metadata.s7zorqm9higc1.m', 'media_metadata.s7zorqm9higc1.p', 'media_metadata.s7zorqm9higc1.s.y', 'media_metadata.s7zorqm9higc1.s.x', 'media_metadata.s7zorqm9higc1.s.u', 'media_metadata.s7zorqm9higc1.id', 'media_metadata.of8wlwc2iigc1.status', 'media_metadata.of8wlwc2iigc1.e', 'media_metadata.of8wlwc2iigc1.m', 'media_metadata.of8wlwc2iigc1.p', 'media_metadata.of8wlwc2iigc1.s.y', 'media_metadata.of8wlwc2iigc1.s.x', 'media_metadata.of8wlwc2iigc1.s.u', 'media_metadata.of8wlwc2iigc1.id', 'media_metadata.6y088bkthigc1.status', 'media_metadata.6y088bkthigc1.e', 'media_metadata.6y088bkthigc1.m', 'media_metadata.6y088bkthigc1.p', 'media_metadata.6y088bkthigc1.s.y', 'media_metadata.6y088bkthigc1.s.x', 'media_metadata.6y088bkthigc1.s.u', 'media_metadata.6y088bkthigc1.id', 'media_metadata.cdtjg51rcujd1.status', 'media_metadata.cdtjg51rcujd1.e', 'media_metadata.cdtjg51rcujd1.m', 'media_metadata.cdtjg51rcujd1.p', 'media_metadata.cdtjg51rcujd1.s.y', 'media_metadata.cdtjg51rcujd1.s.x', 'media_metadata.cdtjg51rcujd1.s.u', 'media_metadata.cdtjg51rcujd1.id', 'media_metadata.jigjbhivs1rc1.status', 'media_metadata.jigjbhivs1rc1.e', 'media_metadata.jigjbhivs1rc1.m', 'media_metadata.jigjbhivs1rc1.p', 'media_metadata.jigjbhivs1rc1.s.y', 'media_metadata.jigjbhivs1rc1.s.x', 'media_metadata.jigjbhivs1rc1.s.u', 'media_metadata.jigjbhivs1rc1.id', 'media_metadata.6lepxkay5bid1.status', 'media_metadata.6lepxkay5bid1.e', 'media_metadata.6lepxkay5bid1.m', 'media_metadata.6lepxkay5bid1.p', 'media_metadata.6lepxkay5bid1.s.y', 'media_metadata.6lepxkay5bid1.s.x', 'media_metadata.6lepxkay5bid1.s.u', 'media_metadata.6lepxkay5bid1.id', 'media_metadata.ddhdxttfea0e1.status', 'media_metadata.ddhdxttfea0e1.e', 'media_metadata.ddhdxttfea0e1.m', 'media_metadata.ddhdxttfea0e1.p', 'media_metadata.ddhdxttfea0e1.s.y', 'media_metadata.ddhdxttfea0e1.s.x', 'media_metadata.ddhdxttfea0e1.s.u', 'media_metadata.ddhdxttfea0e1.id', 'media_metadata.bkte1ttfea0e1.status', 'media_metadata.bkte1ttfea0e1.e', 'media_metadata.bkte1ttfea0e1.m', 'media_metadata.bkte1ttfea0e1.p', 'media_metadata.bkte1ttfea0e1.s.y', 'media_metadata.bkte1ttfea0e1.s.x', 'media_metadata.bkte1ttfea0e1.s.u', 'media_metadata.bkte1ttfea0e1.id', 'media_metadata.irsivadeefjd1.status', 'media_metadata.irsivadeefjd1.e', 'media_metadata.irsivadeefjd1.m', 'media_metadata.irsivadeefjd1.p', 'media_metadata.irsivadeefjd1.s.y', 'media_metadata.irsivadeefjd1.s.x', 'media_metadata.irsivadeefjd1.s.u', 'media_metadata.irsivadeefjd1.id', 'media_metadata.bjmx92u3kwpc1.status', 'media_metadata.bjmx92u3kwpc1.e', 'media_metadata.bjmx92u3kwpc1.m', 'media_metadata.bjmx92u3kwpc1.p', 'media_metadata.bjmx92u3kwpc1.s.y', 'media_metadata.bjmx92u3kwpc1.s.x', 'media_metadata.bjmx92u3kwpc1.s.u', 'media_metadata.bjmx92u3kwpc1.id', 'media_metadata.bteiwtu3kmgd1.status', 'media_metadata.bteiwtu3kmgd1.e', 'media_metadata.bteiwtu3kmgd1.m', 'media_metadata.bteiwtu3kmgd1.p', 'media_metadata.bteiwtu3kmgd1.s.y', 'media_metadata.bteiwtu3kmgd1.s.x', 'media_metadata.bteiwtu3kmgd1.s.u', 'media_metadata.bteiwtu3kmgd1.id', 'media_metadata.vplb9tu3kmgd1.status', 'media_metadata.vplb9tu3kmgd1.e', 'media_metadata.vplb9tu3kmgd1.m', 'media_metadata.vplb9tu3kmgd1.p', 'media_metadata.vplb9tu3kmgd1.s.y', 'media_metadata.vplb9tu3kmgd1.s.x', 'media_metadata.vplb9tu3kmgd1.s.u', 'media_metadata.vplb9tu3kmgd1.id', 'media_metadata.htqqh399t4ad1.status', 'media_metadata.htqqh399t4ad1.e', 'media_metadata.htqqh399t4ad1.m', 'media_metadata.htqqh399t4ad1.p', 'media_metadata.htqqh399t4ad1.s.y', 'media_metadata.htqqh399t4ad1.s.x', 'media_metadata.htqqh399t4ad1.s.u', 'media_metadata.htqqh399t4ad1.id', 'media_metadata.0lt5f0nbt4ad1.status', 'media_metadata.0lt5f0nbt4ad1.e', 'media_metadata.0lt5f0nbt4ad1.m', 'media_metadata.0lt5f0nbt4ad1.p', 'media_metadata.0lt5f0nbt4ad1.s.y', 'media_metadata.0lt5f0nbt4ad1.s.x', 'media_metadata.0lt5f0nbt4ad1.s.u', 'media_metadata.0lt5f0nbt4ad1.id', 'media_metadata.rh8rrqk8t4ad1.status', 'media_metadata.rh8rrqk8t4ad1.e', 'media_metadata.rh8rrqk8t4ad1.m', 'media_metadata.rh8rrqk8t4ad1.p', 'media_metadata.rh8rrqk8t4ad1.s.y', 'media_metadata.rh8rrqk8t4ad1.s.x', 'media_metadata.rh8rrqk8t4ad1.s.u', 'media_metadata.rh8rrqk8t4ad1.id', 'media_metadata.w1m67ikgt4ad1.status', 'media_metadata.w1m67ikgt4ad1.e', 'media_metadata.w1m67ikgt4ad1.m', 'media_metadata.w1m67ikgt4ad1.p', 'media_metadata.w1m67ikgt4ad1.s.y', 'media_metadata.w1m67ikgt4ad1.s.x', 'media_metadata.w1m67ikgt4ad1.s.u', 'media_metadata.w1m67ikgt4ad1.id', 'media_metadata.iff1vcmat4ad1.status', 'media_metadata.iff1vcmat4ad1.e', 'media_metadata.iff1vcmat4ad1.m', 'media_metadata.iff1vcmat4ad1.p', 'media_metadata.iff1vcmat4ad1.s.y', 'media_metadata.iff1vcmat4ad1.s.x', 'media_metadata.iff1vcmat4ad1.s.u', 'media_metadata.iff1vcmat4ad1.id', 'media_metadata.mgbdac9qlaec1.status', 'media_metadata.mgbdac9qlaec1.e', 'media_metadata.mgbdac9qlaec1.m', 'media_metadata.mgbdac9qlaec1.p', 'media_metadata.mgbdac9qlaec1.s.y', 'media_metadata.mgbdac9qlaec1.s.x', 'media_metadata.mgbdac9qlaec1.s.u', 'media_metadata.mgbdac9qlaec1.id', 'media_metadata.qjk5q1fnv58c1.status', 'media_metadata.qjk5q1fnv58c1.e', 'media_metadata.qjk5q1fnv58c1.m', 'media_metadata.qjk5q1fnv58c1.p', 'media_metadata.qjk5q1fnv58c1.s.y', 'media_metadata.qjk5q1fnv58c1.s.x', 'media_metadata.qjk5q1fnv58c1.s.u', 'media_metadata.qjk5q1fnv58c1.id', 'media_metadata.2qy68tawbyqc1.status', 'media_metadata.2qy68tawbyqc1.e', 'media_metadata.2qy68tawbyqc1.m', 'media_metadata.2qy68tawbyqc1.p', 'media_metadata.2qy68tawbyqc1.s.y', 'media_metadata.2qy68tawbyqc1.s.x', 'media_metadata.2qy68tawbyqc1.s.u', 'media_metadata.2qy68tawbyqc1.id', 'media_metadata.piqqv9nrgl7c1.status', 'media_metadata.piqqv9nrgl7c1.e', 'media_metadata.piqqv9nrgl7c1.m', 'media_metadata.piqqv9nrgl7c1.p', 'media_metadata.piqqv9nrgl7c1.s.y', 'media_metadata.piqqv9nrgl7c1.s.x', 'media_metadata.piqqv9nrgl7c1.s.u', 'media_metadata.piqqv9nrgl7c1.id', 'media_metadata.uqnuo8fpgl7c1.status', 'media_metadata.uqnuo8fpgl7c1.e', 'media_metadata.uqnuo8fpgl7c1.m', 'media_metadata.uqnuo8fpgl7c1.p', 'media_metadata.uqnuo8fpgl7c1.s.y', 'media_metadata.uqnuo8fpgl7c1.s.x', 'media_metadata.uqnuo8fpgl7c1.s.u', 'media_metadata.uqnuo8fpgl7c1.id', 'media_metadata.okhwyg75gl7c1.status', 'media_metadata.okhwyg75gl7c1.e', 'media_metadata.okhwyg75gl7c1.m', 'media_metadata.okhwyg75gl7c1.p', 'media_metadata.okhwyg75gl7c1.s.y', 'media_metadata.okhwyg75gl7c1.s.x', 'media_metadata.okhwyg75gl7c1.s.u', 'media_metadata.okhwyg75gl7c1.id', 'media_metadata.53kl3padhl7c1.status', 'media_metadata.53kl3padhl7c1.e', 'media_metadata.53kl3padhl7c1.m', 'media_metadata.53kl3padhl7c1.p', 'media_metadata.53kl3padhl7c1.s.y', 'media_metadata.53kl3padhl7c1.s.x', 'media_metadata.53kl3padhl7c1.s.u', 'media_metadata.53kl3padhl7c1.id', 'media_metadata.h0g0fnrqup5c1.status', 'media_metadata.h0g0fnrqup5c1.e', 'media_metadata.h0g0fnrqup5c1.m', 'media_metadata.h0g0fnrqup5c1.p', 'media_metadata.h0g0fnrqup5c1.s.y', 'media_metadata.h0g0fnrqup5c1.s.x', 'media_metadata.h0g0fnrqup5c1.s.u', 'media_metadata.h0g0fnrqup5c1.id', 'media_metadata.r9dczx0vup5c1.status', 'media_metadata.r9dczx0vup5c1.e', 'media_metadata.r9dczx0vup5c1.m', 'media_metadata.r9dczx0vup5c1.p', 'media_metadata.r9dczx0vup5c1.s.y', 'media_metadata.r9dczx0vup5c1.s.x', 'media_metadata.r9dczx0vup5c1.s.u', 'media_metadata.r9dczx0vup5c1.id', 'media_metadata.fzwyyx2hs5nc1.status', 'media_metadata.fzwyyx2hs5nc1.e', 'media_metadata.fzwyyx2hs5nc1.m', 'media_metadata.fzwyyx2hs5nc1.p', 'media_metadata.fzwyyx2hs5nc1.s.y', 'media_metadata.fzwyyx2hs5nc1.s.x', 'media_metadata.fzwyyx2hs5nc1.s.u', 'media_metadata.fzwyyx2hs5nc1.id', 'media_metadata.oyj3x3p3t5nc1.status', 'media_metadata.oyj3x3p3t5nc1.e', 'media_metadata.oyj3x3p3t5nc1.m', 'media_metadata.oyj3x3p3t5nc1.p', 'media_metadata.oyj3x3p3t5nc1.s.y', 'media_metadata.oyj3x3p3t5nc1.s.x', 'media_metadata.oyj3x3p3t5nc1.s.u', 'media_metadata.oyj3x3p3t5nc1.id', 'media_metadata.bbvirn8ywhbc1.status', 'media_metadata.bbvirn8ywhbc1.e', 'media_metadata.bbvirn8ywhbc1.m', 'media_metadata.bbvirn8ywhbc1.p', 'media_metadata.bbvirn8ywhbc1.s.y', 'media_metadata.bbvirn8ywhbc1.s.x', 'media_metadata.bbvirn8ywhbc1.s.u', 'media_metadata.bbvirn8ywhbc1.id', 'media_metadata.cwrkfh8scmqc1.status', 'media_metadata.cwrkfh8scmqc1.e', 'media_metadata.cwrkfh8scmqc1.m', 'media_metadata.cwrkfh8scmqc1.p', 'media_metadata.cwrkfh8scmqc1.s.y', 'media_metadata.cwrkfh8scmqc1.s.x', 'media_metadata.cwrkfh8scmqc1.s.u', 'media_metadata.cwrkfh8scmqc1.id', 'media_metadata.vevipqig659d1.status', 'media_metadata.vevipqig659d1.e', 'media_metadata.vevipqig659d1.m', 'media_metadata.vevipqig659d1.p', 'media_metadata.vevipqig659d1.s.y', 'media_metadata.vevipqig659d1.s.x', 'media_metadata.vevipqig659d1.s.u', 'media_metadata.vevipqig659d1.id', 'media_metadata.9zj1pduy5m0e1.status', 'media_metadata.9zj1pduy5m0e1.e', 'media_metadata.9zj1pduy5m0e1.m', 'media_metadata.9zj1pduy5m0e1.p', 'media_metadata.9zj1pduy5m0e1.s.y', 'media_metadata.9zj1pduy5m0e1.s.x', 'media_metadata.9zj1pduy5m0e1.s.u', 'media_metadata.9zj1pduy5m0e1.id', 'media_metadata.gxffozklzttd1.status', 'media_metadata.gxffozklzttd1.e', 'media_metadata.gxffozklzttd1.m', 'media_metadata.gxffozklzttd1.p', 'media_metadata.gxffozklzttd1.s.y', 'media_metadata.gxffozklzttd1.s.x', 'media_metadata.gxffozklzttd1.s.u', 'media_metadata.gxffozklzttd1.id', 'media_metadata.xnbeqyllzttd1.status', 'media_metadata.xnbeqyllzttd1.e', 'media_metadata.xnbeqyllzttd1.m', 'media_metadata.xnbeqyllzttd1.p', 'media_metadata.xnbeqyllzttd1.s.y', 'media_metadata.xnbeqyllzttd1.s.x', 'media_metadata.xnbeqyllzttd1.s.u', 'media_metadata.xnbeqyllzttd1.id', 'media_metadata.do87z0llzttd1.status', 'media_metadata.do87z0llzttd1.e', 'media_metadata.do87z0llzttd1.m', 'media_metadata.do87z0llzttd1.p', 'media_metadata.do87z0llzttd1.s.y', 'media_metadata.do87z0llzttd1.s.x', 'media_metadata.do87z0llzttd1.s.u', 'media_metadata.do87z0llzttd1.id', 'media_metadata.b2egsxllzttd1.status', 'media_metadata.b2egsxllzttd1.e', 'media_metadata.b2egsxllzttd1.m', 'media_metadata.b2egsxllzttd1.p', 'media_metadata.b2egsxllzttd1.s.y', 'media_metadata.b2egsxllzttd1.s.x', 'media_metadata.b2egsxllzttd1.s.u', 'media_metadata.b2egsxllzttd1.id', 'media_metadata.ghh20xllzttd1.status', 'media_metadata.ghh20xllzttd1.e', 'media_metadata.ghh20xllzttd1.m', 'media_metadata.ghh20xllzttd1.p', 'media_metadata.ghh20xllzttd1.s.y', 'media_metadata.ghh20xllzttd1.s.x', 'media_metadata.ghh20xllzttd1.s.u', 'media_metadata.ghh20xllzttd1.id', 'media_metadata.nwvr7dt4po5c1.status', 'media_metadata.nwvr7dt4po5c1.e', 'media_metadata.nwvr7dt4po5c1.m', 'media_metadata.nwvr7dt4po5c1.p', 'media_metadata.nwvr7dt4po5c1.s.y', 'media_metadata.nwvr7dt4po5c1.s.x', 'media_metadata.nwvr7dt4po5c1.s.u', 'media_metadata.nwvr7dt4po5c1.id', 'media_metadata.juvh4x4thhjc1.status', 'media_metadata.juvh4x4thhjc1.e', 'media_metadata.juvh4x4thhjc1.m', 'media_metadata.juvh4x4thhjc1.p', 'media_metadata.juvh4x4thhjc1.s.y', 'media_metadata.juvh4x4thhjc1.s.x', 'media_metadata.juvh4x4thhjc1.s.u', 'media_metadata.juvh4x4thhjc1.id', 'media_metadata.mxznjgrwt8sd1.status', 'media_metadata.mxznjgrwt8sd1.e', 'media_metadata.mxznjgrwt8sd1.m', 'media_metadata.mxznjgrwt8sd1.p', 'media_metadata.mxznjgrwt8sd1.s.y', 'media_metadata.mxznjgrwt8sd1.s.gif', 'media_metadata.mxznjgrwt8sd1.s.mp4', 'media_metadata.mxznjgrwt8sd1.s.x', 'media_metadata.mxznjgrwt8sd1.id', 'media_metadata.4mgtdzjsa2nc1.status', 'media_metadata.4mgtdzjsa2nc1.e', 'media_metadata.4mgtdzjsa2nc1.m', 'media_metadata.4mgtdzjsa2nc1.p', 'media_metadata.4mgtdzjsa2nc1.s.y', 'media_metadata.4mgtdzjsa2nc1.s.x', 'media_metadata.4mgtdzjsa2nc1.s.u', 'media_metadata.4mgtdzjsa2nc1.id', 'media_metadata.vyfy743jab5c1.status', 'media_metadata.vyfy743jab5c1.e', 'media_metadata.vyfy743jab5c1.m', 'media_metadata.vyfy743jab5c1.p', 'media_metadata.vyfy743jab5c1.s.y', 'media_metadata.vyfy743jab5c1.s.x', 'media_metadata.vyfy743jab5c1.s.u', 'media_metadata.vyfy743jab5c1.id', 'media_metadata.gaaeouimab5c1.status', 'media_metadata.gaaeouimab5c1.e', 'media_metadata.gaaeouimab5c1.m', 'media_metadata.gaaeouimab5c1.p', 'media_metadata.gaaeouimab5c1.s.y', 'media_metadata.gaaeouimab5c1.s.x', 'media_metadata.gaaeouimab5c1.s.u', 'media_metadata.gaaeouimab5c1.id', 'media_metadata.pp9vll5o0lqc1.status', 'media_metadata.pp9vll5o0lqc1.e', 'media_metadata.pp9vll5o0lqc1.m', 'media_metadata.pp9vll5o0lqc1.p', 'media_metadata.pp9vll5o0lqc1.s.y', 'media_metadata.pp9vll5o0lqc1.s.x', 'media_metadata.pp9vll5o0lqc1.s.u', 'media_metadata.pp9vll5o0lqc1.id', 'media_metadata.3hyu6a8jhhlc1.status', 'media_metadata.3hyu6a8jhhlc1.e', 'media_metadata.3hyu6a8jhhlc1.m', 'media_metadata.3hyu6a8jhhlc1.p', 'media_metadata.3hyu6a8jhhlc1.s.y', 'media_metadata.3hyu6a8jhhlc1.s.x', 'media_metadata.3hyu6a8jhhlc1.s.u', 'media_metadata.3hyu6a8jhhlc1.id', 'media_metadata.hu2cpyo26qmc1.status', 'media_metadata.hu2cpyo26qmc1.e', 'media_metadata.hu2cpyo26qmc1.m', 'media_metadata.hu2cpyo26qmc1.p', 'media_metadata.hu2cpyo26qmc1.s.y', 'media_metadata.hu2cpyo26qmc1.s.x', 'media_metadata.hu2cpyo26qmc1.s.u', 'media_metadata.hu2cpyo26qmc1.id', 'media_metadata.ve6753p75yqc1.status', 'media_metadata.ve6753p75yqc1.e', 'media_metadata.ve6753p75yqc1.m', 'media_metadata.ve6753p75yqc1.p', 'media_metadata.ve6753p75yqc1.s.y', 'media_metadata.ve6753p75yqc1.s.x', 'media_metadata.ve6753p75yqc1.s.u', 'media_metadata.ve6753p75yqc1.id', 'media_metadata.9zblqwmz8b0e1.status', 'media_metadata.9zblqwmz8b0e1.e', 'media_metadata.9zblqwmz8b0e1.m', 'media_metadata.9zblqwmz8b0e1.p', 'media_metadata.9zblqwmz8b0e1.s.y', 'media_metadata.9zblqwmz8b0e1.s.x', 'media_metadata.9zblqwmz8b0e1.s.u', 'media_metadata.9zblqwmz8b0e1.id', 'secure_media.oembed.description', 'secure_media.oembed.mean_alpha', 'media.oembed.description', 'media.oembed.mean_alpha', 'media_metadata.8u50wr1mo7ic1.status', 'media_metadata.8u50wr1mo7ic1.e', 'media_metadata.8u50wr1mo7ic1.m', 'media_metadata.8u50wr1mo7ic1.p', 'media_metadata.8u50wr1mo7ic1.s.y', 'media_metadata.8u50wr1mo7ic1.s.x', 'media_metadata.8u50wr1mo7ic1.s.u', 'media_metadata.8u50wr1mo7ic1.id']\n"
     ]
    }
   ],
   "source": [
    "print(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Columns: 105 entries, approved_at_utc to author_cakeday\n",
      "dtypes: bool(27), float64(5), int64(10), object(63)\n",
      "memory usage: 635.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# We got rid of a bunch of columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify columns that have weird data types (or are constant)\n",
    "\n",
    "For each column, check if it is made up primarily of lists (Return the string 'list' if it is).\n",
    "\n",
    "If not, check if it has just one single value (Return the string 'single' if it is). Otherwise, return the string 'multiple'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "column_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "columns",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "cba82870-b530-48e6-be91-ad6cb2162cc7",
       "rows": [
        [
         "list",
         "8",
         "['link_flair_richtext', 'user_reports', 'author_flair_richtext', 'all_awardings', 'awarders', 'treatment_tags', 'mod_reports', 'crosspost_parent_list']"
        ],
        [
         "multiple",
         "59",
         "['approved_at_utc', 'selftext', 'author_fullname', 'mod_reason_title', 'title', 'link_flair_css_class', 'thumbnail_height', 'top_awarded_type', 'name', 'link_flair_text_color', 'upvote_ratio', 'author_flair_background_color', 'ups', 'is_original_content', 'is_reddit_media_domain', 'category', 'link_flair_text', 'score', 'approved_by', 'author_premium', 'thumbnail', 'edited', 'author_flair_css_class', 'post_hint', 'content_categories', 'is_self', 'created', 'removed_by_category', 'banned_by', 'domain', 'allow_live_comments', 'selftext_html', 'likes', 'banned_at_utc', 'url_overridden_by_dest', 'view_count', 'archived', 'over_18', 'link_flair_template_id', 'locked', 'author_flair_text', 'removed_by', 'mod_note', 'mod_reason_by', 'num_reports', 'removal_reason', 'link_flair_background_color', 'id', 'report_reasons', 'author', 'discussion_type', 'num_comments', 'send_replies', 'permalink', 'url', 'subreddit_subscribers', 'created_utc', 'num_crossposts', 'crosspost_parent']"
        ],
        [
         "single",
         "38",
         "['subreddit', 'saved', 'gilded', 'clicked', 'subreddit_name_prefixed', 'hidden', 'pwls', 'downs', 'hide_score', 'quarantine', 'total_awards_received', 'thumbnail_width', 'author_flair_template_id', 'is_meta', 'can_mod_post', 'is_created_from_ads_ui', 'subreddit_type', 'link_flair_type', 'wls', 'author_flair_type', 'suggested_sort', 'no_follow', 'is_crosspostable', 'pinned', 'can_gild', 'spoiler', 'visited', 'distinguished', 'subreddit_id', 'author_is_blocked', 'is_robot_indexable', 'contest_mode', 'author_patreon_flair', 'author_flair_text_color', 'stickied', 'is_video', 'is_gallery', 'author_cakeday']"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>columns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>list</th>\n",
       "      <td>8</td>\n",
       "      <td>[link_flair_richtext, user_reports, author_fla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multiple</th>\n",
       "      <td>59</td>\n",
       "      <td>[approved_at_utc, selftext, author_fullname, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single</th>\n",
       "      <td>38</td>\n",
       "      <td>[subreddit, saved, gilded, clicked, subreddit_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count                                            columns\n",
       "column_type                                                          \n",
       "list             8  [link_flair_richtext, user_reports, author_fla...\n",
       "multiple        59  [approved_at_utc, selftext, author_fullname, m...\n",
       "single          38  [subreddit, saved, gilded, clicked, subreddit_..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_column_type(column):\n",
    "    \"\"\"\n",
    "    Determine the type of a column based on its values.\n",
    "\n",
    "    Parameters:\n",
    "        column (pandas.Series): The column to analyze.\n",
    "\n",
    "    Returns:\n",
    "        str: The type of the column. Possible values are 'list', 'single', or 'multiple'.\n",
    "    \"\"\"\n",
    "\n",
    "    if any(isinstance(i, list) for i in column):\n",
    "        return 'list'\n",
    "    elif column.nunique() == 1:\n",
    "        return 'single'\n",
    "    else:\n",
    "        return 'multiple'\n",
    "\n",
    "# Let's see what types of columns we have\n",
    "survey_column_types = (\n",
    "    df.apply(get_column_type)\n",
    "      .reset_index(name='column_type')\n",
    "      .rename(columns={'index': 'column'})\n",
    "      .groupby(['column_type'])\n",
    "      .apply(lambda x: pd.Series({'count': len(x), 'columns': x['column'].tolist()}),\n",
    "             include_groups=False)\n",
    ")\n",
    "\n",
    "# After inspecting, I decided that I only care about the columns that have multiple values\n",
    "survey_column_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some manual investigation, I decided to keep only the columns that have multiple values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_value_columns = survey_column_types.loc['multiple', 'columns']\n",
    "\n",
    "# We use copy to avoid SettingDWithCopyWarning later on\n",
    "df = df[multiple_value_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It should be easier to select for what we want now\n",
    "\n",
    "After further inspection of the remaining columns, I decided to keep only the following columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns =  [\n",
    "    'id', 'title', 'permalink', 'post_hint',  'url', 'created_utc',           # Identifiers \n",
    "    'ups', 'upvote_ratio', 'score',                                           # Votes\n",
    "    'num_comments', 'is_original_content', 'is_self',                         # Post metadata\n",
    "    'author', 'author_fullname',                                              # Author metadata\n",
    "    'content_categories', 'edited', 'domain'                                  # Misc metadata\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use copy to avoid SettingDWithCopyWarning later on\n",
    "df = df[selected_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Cleaning up the DataFrame\n",
    "\n",
    "After A LOT OF WORK, I can summarise my entire initial cleaning of the DataFrame in the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "permalink",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "post_hint",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "created_utc",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "ups",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "upvote_ratio",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "num_comments",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "is_original_content",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "is_self",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "author",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "author_fullname",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "content_categories",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "edited",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "domain",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "01d168b7-24c0-46ab-a4bb-eb137c984407",
       "rows": [
        [
         "https://www.reddit.com/r/datascience/comments/1f6ztk4/how_to_avoid_12assed_data_analysis/",
         "1f6ztk4",
         "How to avoid 1/2-assed data analysis",
         "image",
         "https://i.redd.it/l6zd3zsa9cmd1.jpeg",
         "2024-09-02 07:18:04",
         "3122",
         "0.98",
         "3122",
         "56",
         "False",
         "False",
         "BdR76",
         "t2_sejrw",
         null,
         "False",
         "i.redd.it"
        ],
        [
         "https://www.reddit.com/r/datascience/comments/1feyx6h/favourite_piece_of_code/",
         "1feyx6h",
         "Favourite piece of code ü§£",
         "image",
         "https://i.redd.it/jqvbe1ycrcod1.jpeg",
         "2024-09-12 11:06:40",
         "2810",
         "0.98",
         "2810",
         "102",
         "False",
         "False",
         "nobody_undefined",
         "t2_dh9o0mnfv",
         null,
         "False",
         "i.redd.it"
        ],
        [
         "https://www.reddit.com/r/datascience/comments/1dsnbww/youre_not_helping_excel_please_stop_helping/",
         "1dsnbww",
         "You're not helping, Excel! please STOP HELPING!!!",
         "image",
         "https://i.redd.it/ccxo0ajs3v9d1.jpeg",
         "2024-07-01 08:46:27",
         "1808",
         "0.97",
         "1808",
         "155",
         "False",
         "False",
         "BdR76",
         "t2_sejrw",
         null,
         "False",
         "i.redd.it"
        ],
        [
         "https://www.reddit.com/r/datascience/comments/1afgvv9/friendly_reminder_not_to_work_too_hard_youll_just/",
         "1afgvv9",
         "Friendly reminder not to work too hard. You'll just get fired",
         null,
         "https://www.reddit.com/r/datascience/comments/1afgvv9/friendly_reminder_not_to_work_too_hard_youll_just/",
         "2024-01-31 12:37:25",
         "1704",
         "0.93",
         "1704",
         "204",
         "False",
         "True",
         "[deleted]",
         null,
         null,
         "False",
         "self.datascience"
        ],
        [
         "https://www.reddit.com/r/datascience/comments/1b1au2f/data_scientist_quits_her_job_at_spotify/",
         "1b1au2f",
         "Data scientist quits her job at Spotify",
         "rich:video",
         "https://youtu.be/OMI4Wu9wnY0?si=teFkXgTnPmUAuAyU",
         "2024-02-27 12:37:07",
         "1398",
         "0.91",
         "1398",
         "373",
         "False",
         "False",
         "Direct-Touch469",
         "t2_uy28jztl",
         null,
         "False",
         "youtu.be"
        ]
       ],
       "shape": {
        "columns": 16,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>post_hint</th>\n",
       "      <th>url</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>ups</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>is_self</th>\n",
       "      <th>author</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>content_categories</th>\n",
       "      <th>edited</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>permalink</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>https://www.reddit.com/r/datascience/comments/1f6ztk4/how_to_avoid_12assed_data_analysis/</th>\n",
       "      <td>1f6ztk4</td>\n",
       "      <td>How to avoid 1/2-assed data analysis</td>\n",
       "      <td>image</td>\n",
       "      <td>https://i.redd.it/l6zd3zsa9cmd1.jpeg</td>\n",
       "      <td>2024-09-02 07:18:04</td>\n",
       "      <td>3122</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3122</td>\n",
       "      <td>56</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>BdR76</td>\n",
       "      <td>t2_sejrw</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>i.redd.it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.reddit.com/r/datascience/comments/1feyx6h/favourite_piece_of_code/</th>\n",
       "      <td>1feyx6h</td>\n",
       "      <td>Favourite piece of code ü§£</td>\n",
       "      <td>image</td>\n",
       "      <td>https://i.redd.it/jqvbe1ycrcod1.jpeg</td>\n",
       "      <td>2024-09-12 11:06:40</td>\n",
       "      <td>2810</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2810</td>\n",
       "      <td>102</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>nobody_undefined</td>\n",
       "      <td>t2_dh9o0mnfv</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>i.redd.it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.reddit.com/r/datascience/comments/1dsnbww/youre_not_helping_excel_please_stop_helping/</th>\n",
       "      <td>1dsnbww</td>\n",
       "      <td>You're not helping, Excel! please STOP HELPING!!!</td>\n",
       "      <td>image</td>\n",
       "      <td>https://i.redd.it/ccxo0ajs3v9d1.jpeg</td>\n",
       "      <td>2024-07-01 08:46:27</td>\n",
       "      <td>1808</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1808</td>\n",
       "      <td>155</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>BdR76</td>\n",
       "      <td>t2_sejrw</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>i.redd.it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.reddit.com/r/datascience/comments/1afgvv9/friendly_reminder_not_to_work_too_hard_youll_just/</th>\n",
       "      <td>1afgvv9</td>\n",
       "      <td>Friendly reminder not to work too hard. You'll...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>2024-01-31 12:37:25</td>\n",
       "      <td>1704</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1704</td>\n",
       "      <td>204</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>self.datascience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.reddit.com/r/datascience/comments/1b1au2f/data_scientist_quits_her_job_at_spotify/</th>\n",
       "      <td>1b1au2f</td>\n",
       "      <td>Data scientist quits her job at Spotify</td>\n",
       "      <td>rich:video</td>\n",
       "      <td>https://youtu.be/OMI4Wu9wnY0?si=teFkXgTnPmUAuAyU</td>\n",
       "      <td>2024-02-27 12:37:07</td>\n",
       "      <td>1398</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1398</td>\n",
       "      <td>373</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Direct-Touch469</td>\n",
       "      <td>t2_uy28jztl</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>youtu.be</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         id  \\\n",
       "permalink                                                     \n",
       "https://www.reddit.com/r/datascience/comments/1...  1f6ztk4   \n",
       "https://www.reddit.com/r/datascience/comments/1...  1feyx6h   \n",
       "https://www.reddit.com/r/datascience/comments/1...  1dsnbww   \n",
       "https://www.reddit.com/r/datascience/comments/1...  1afgvv9   \n",
       "https://www.reddit.com/r/datascience/comments/1...  1b1au2f   \n",
       "\n",
       "                                                                                                title  \\\n",
       "permalink                                                                                               \n",
       "https://www.reddit.com/r/datascience/comments/1...               How to avoid 1/2-assed data analysis   \n",
       "https://www.reddit.com/r/datascience/comments/1...                          Favourite piece of code ü§£   \n",
       "https://www.reddit.com/r/datascience/comments/1...  You're not helping, Excel! please STOP HELPING!!!   \n",
       "https://www.reddit.com/r/datascience/comments/1...  Friendly reminder not to work too hard. You'll...   \n",
       "https://www.reddit.com/r/datascience/comments/1...            Data scientist quits her job at Spotify   \n",
       "\n",
       "                                                     post_hint  \\\n",
       "permalink                                                        \n",
       "https://www.reddit.com/r/datascience/comments/1...       image   \n",
       "https://www.reddit.com/r/datascience/comments/1...       image   \n",
       "https://www.reddit.com/r/datascience/comments/1...       image   \n",
       "https://www.reddit.com/r/datascience/comments/1...         NaN   \n",
       "https://www.reddit.com/r/datascience/comments/1...  rich:video   \n",
       "\n",
       "                                                                                                  url  \\\n",
       "permalink                                                                                               \n",
       "https://www.reddit.com/r/datascience/comments/1...               https://i.redd.it/l6zd3zsa9cmd1.jpeg   \n",
       "https://www.reddit.com/r/datascience/comments/1...               https://i.redd.it/jqvbe1ycrcod1.jpeg   \n",
       "https://www.reddit.com/r/datascience/comments/1...               https://i.redd.it/ccxo0ajs3v9d1.jpeg   \n",
       "https://www.reddit.com/r/datascience/comments/1...  https://www.reddit.com/r/datascience/comments/...   \n",
       "https://www.reddit.com/r/datascience/comments/1...   https://youtu.be/OMI4Wu9wnY0?si=teFkXgTnPmUAuAyU   \n",
       "\n",
       "                                                           created_utc   ups  \\\n",
       "permalink                                                                      \n",
       "https://www.reddit.com/r/datascience/comments/1... 2024-09-02 07:18:04  3122   \n",
       "https://www.reddit.com/r/datascience/comments/1... 2024-09-12 11:06:40  2810   \n",
       "https://www.reddit.com/r/datascience/comments/1... 2024-07-01 08:46:27  1808   \n",
       "https://www.reddit.com/r/datascience/comments/1... 2024-01-31 12:37:25  1704   \n",
       "https://www.reddit.com/r/datascience/comments/1... 2024-02-27 12:37:07  1398   \n",
       "\n",
       "                                                    upvote_ratio  score  \\\n",
       "permalink                                                                 \n",
       "https://www.reddit.com/r/datascience/comments/1...          0.98   3122   \n",
       "https://www.reddit.com/r/datascience/comments/1...          0.98   2810   \n",
       "https://www.reddit.com/r/datascience/comments/1...          0.97   1808   \n",
       "https://www.reddit.com/r/datascience/comments/1...          0.93   1704   \n",
       "https://www.reddit.com/r/datascience/comments/1...          0.91   1398   \n",
       "\n",
       "                                                    num_comments  \\\n",
       "permalink                                                          \n",
       "https://www.reddit.com/r/datascience/comments/1...            56   \n",
       "https://www.reddit.com/r/datascience/comments/1...           102   \n",
       "https://www.reddit.com/r/datascience/comments/1...           155   \n",
       "https://www.reddit.com/r/datascience/comments/1...           204   \n",
       "https://www.reddit.com/r/datascience/comments/1...           373   \n",
       "\n",
       "                                                    is_original_content  \\\n",
       "permalink                                                                 \n",
       "https://www.reddit.com/r/datascience/comments/1...                False   \n",
       "https://www.reddit.com/r/datascience/comments/1...                False   \n",
       "https://www.reddit.com/r/datascience/comments/1...                False   \n",
       "https://www.reddit.com/r/datascience/comments/1...                False   \n",
       "https://www.reddit.com/r/datascience/comments/1...                False   \n",
       "\n",
       "                                                    is_self            author  \\\n",
       "permalink                                                                       \n",
       "https://www.reddit.com/r/datascience/comments/1...    False             BdR76   \n",
       "https://www.reddit.com/r/datascience/comments/1...    False  nobody_undefined   \n",
       "https://www.reddit.com/r/datascience/comments/1...    False             BdR76   \n",
       "https://www.reddit.com/r/datascience/comments/1...     True         [deleted]   \n",
       "https://www.reddit.com/r/datascience/comments/1...    False   Direct-Touch469   \n",
       "\n",
       "                                                   author_fullname  \\\n",
       "permalink                                                            \n",
       "https://www.reddit.com/r/datascience/comments/1...        t2_sejrw   \n",
       "https://www.reddit.com/r/datascience/comments/1...    t2_dh9o0mnfv   \n",
       "https://www.reddit.com/r/datascience/comments/1...        t2_sejrw   \n",
       "https://www.reddit.com/r/datascience/comments/1...             NaN   \n",
       "https://www.reddit.com/r/datascience/comments/1...     t2_uy28jztl   \n",
       "\n",
       "                                                   content_categories edited  \\\n",
       "permalink                                                                      \n",
       "https://www.reddit.com/r/datascience/comments/1...               None  False   \n",
       "https://www.reddit.com/r/datascience/comments/1...               None  False   \n",
       "https://www.reddit.com/r/datascience/comments/1...               None  False   \n",
       "https://www.reddit.com/r/datascience/comments/1...               None  False   \n",
       "https://www.reddit.com/r/datascience/comments/1...               None  False   \n",
       "\n",
       "                                                              domain  \n",
       "permalink                                                             \n",
       "https://www.reddit.com/r/datascience/comments/1...         i.redd.it  \n",
       "https://www.reddit.com/r/datascience/comments/1...         i.redd.it  \n",
       "https://www.reddit.com/r/datascience/comments/1...         i.redd.it  \n",
       "https://www.reddit.com/r/datascience/comments/1...  self.datascience  \n",
       "https://www.reddit.com/r/datascience/comments/1...          youtu.be  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Points to where the data is stored\n",
    "filepath = '../data/reddit/raw/datascience_top1000_2024_11_21_09_56.json'\n",
    "\n",
    "\n",
    "# Columns that I identified as relevant\n",
    "selected_columns =     [\n",
    "    'id', 'title', 'permalink', 'post_hint',  'url', 'created_utc',            # Identifiers \n",
    "     'ups', 'upvote_ratio', 'score',                                           # Votes\n",
    "     'num_comments', 'is_original_content', 'is_self',                         # Post metadata\n",
    "     'author', 'author_fullname',                                              # Author metadata\n",
    "     'content_categories', 'edited', 'domain']                                 # Misc metadata\n",
    "\n",
    "df = (\n",
    "    pd.json_normalize(pd.read_json(filepath)['data'])\n",
    "\n",
    "    # Reformat the `created_utc` column to a human-readable format\n",
    "    .assign(created_utc = lambda x: x['created_utc'].apply(lambda x: datetime.fromtimestamp(x)),\n",
    "            permalink = lambda x: x['permalink'].apply(lambda x: f\"https://www.reddit.com{x}\"))\n",
    "\n",
    "    # Select only the columns that I identified as relevant\n",
    "    .loc[:, selected_columns]\n",
    ")\n",
    "\n",
    "# The top 5 posts\n",
    "# Set an index just to make it easier for me to click on the links\n",
    "df.head(5).set_index('permalink')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Moving this to a database\n",
    "\n",
    "We've learned about CSV, JSON, and plain text (.txt) files. These are all valid ways to store data, but if you see your data growing and need access quickly, you might consider using a **database**.\n",
    "\n",
    "\n",
    "## 3.1 What is a database?\n",
    "\n",
    "As this is not a database course, I will focus only on the basics. We will use a simple database called **SQLite**. It is good for us because it is simple and built into Python.\n",
    "\n",
    "SQLite is:\n",
    "\n",
    "- self-contained, \n",
    "- serverless,\n",
    "- zero-configuration,\n",
    "- transactional SQL database engine. \n",
    "\n",
    "As usual, there is a good W3Schools tutorial on [SQLite](https://www.w3schools.com/sql/sql_intro.asp).\n",
    "\n",
    "<div style=\"width:70%;border: 1px solid #aaa; border-radius:1em; padding: 1em; margin: 1em 0;\">\n",
    "\n",
    "\n",
    "üå∞ **In a nutshell: what is a relational (SQL) database?**\n",
    "\n",
    "- Think of it as an even more structured way to store tables (data frames).\n",
    "\n",
    "- If you have a huge table, it is a much more efficient way to store and query data.\n",
    "\n",
    "- Merges are much faster than in üêº `pandas` if you have a lot of data.\n",
    "\n",
    "- Every **tuple** (nearly the same concept of a row) in a table is uniquely identified by a **primary key**, a column, or a set of columns that uniquely identifies a row.\n",
    "\n",
    "- Tables can be related to each other using **foreign keys**. This is a way to link tables together.\n",
    "\n",
    "- SQL stands for **Structured Query Language**. It is a language used to interact with databases. You can use it to create, read, update, and delete data from a database.\n",
    "\n",
    "- Most of üêº `pandas` functions are essentially an adaptation of SQL\n",
    "\n",
    "- SQLite, the database we will use, is a simple database that comes installed with Python.\n",
    "\n",
    "- ‚≠ê SQLite can store multiple tables in a single file.\n",
    "\n",
    "- A SQLite database file can grow to be as large as about 281 terabytes\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Connecting and exporting the DataFrame to a SQLite database\n",
    "\n",
    "<font style=\"color:#e26a4f;font-weight:bold;\">**THIS IS USEFUL TO KNOW BUT NOT THE WAY WE WANT YOU TO DO IT. SEE SECTION 4.**</font>\n",
    "\n",
    "üêº `pandas` allows you to export your dataframe to a SQLite database using the [pd.to_sql() function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html):\n",
    "\n",
    "```python\n",
    "\n",
    "df.to_sql('table_name', con=connection_object)\n",
    "\n",
    "```\n",
    "\n",
    "But first, as indicated above, we need to establish a connection to the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Establishing a connection to a database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a database engine using SQLAlchemy\n",
    "engine = create_engine('sqlite:///../data/reddit/reddit.db', echo=False, isolation_level=\"AUTOCOMMIT\")\n",
    "\n",
    "# Why? Read: https://stackoverflow.com/a/71685414/843365\n",
    "with engine.connect() as conn:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once you have a connection to a database, storing a DataFrame in it is as simple as:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_sql('top_posts', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a VSCode extension, like [SQLite Viewer](https://marketplace.visualstudio.com/items?itemName=qwtel.sqlite-viewer) you can easily inspect the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Overwriting the table\n",
    "\n",
    "If you made a mistake or changed your mind about the data you stored in the database, you can overwrite the table by setting the `if_exists` parameter to `'replace'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, I realised that my DataFrame did not store the rank of the post. I can fix the DataFrame in Python and then overwrite the table in the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: It would be better to do this in the function that reads the data, not here.\n",
    "df['rank'] = range(1, df.shape[0] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_sql('top_posts', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Reading from a database\n",
    "\n",
    "If you have a connection to a database, you can read a table from it using the `pd.read_sql()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_db = pd.read_sql('top_posts', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "permalink",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "post_hint",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "created_utc",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "ups",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "upvote_ratio",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "num_comments",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "is_original_content",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "is_self",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "author",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "author_fullname",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "content_categories",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "edited",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "domain",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rank",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "f67bd36c-3d3e-480f-933a-62a406eb920c",
       "rows": [
        [
         "0",
         "1f6ztk4",
         "How to avoid 1/2-assed data analysis",
         "https://www.reddit.com/r/datascience/comments/1f6ztk4/how_to_avoid_12assed_data_analysis/",
         "image",
         "https://i.redd.it/l6zd3zsa9cmd1.jpeg",
         "2024-09-02 07:18:04",
         "3122",
         "0.98",
         "3122",
         "56",
         "False",
         "False",
         "BdR76",
         "t2_sejrw",
         null,
         "0",
         "i.redd.it",
         "1"
        ],
        [
         "1",
         "1feyx6h",
         "Favourite piece of code ü§£",
         "https://www.reddit.com/r/datascience/comments/1feyx6h/favourite_piece_of_code/",
         "image",
         "https://i.redd.it/jqvbe1ycrcod1.jpeg",
         "2024-09-12 11:06:40",
         "2810",
         "0.98",
         "2810",
         "102",
         "False",
         "False",
         "nobody_undefined",
         "t2_dh9o0mnfv",
         null,
         "0",
         "i.redd.it",
         "2"
        ],
        [
         "2",
         "1dsnbww",
         "You're not helping, Excel! please STOP HELPING!!!",
         "https://www.reddit.com/r/datascience/comments/1dsnbww/youre_not_helping_excel_please_stop_helping/",
         "image",
         "https://i.redd.it/ccxo0ajs3v9d1.jpeg",
         "2024-07-01 08:46:27",
         "1808",
         "0.97",
         "1808",
         "155",
         "False",
         "False",
         "BdR76",
         "t2_sejrw",
         null,
         "0",
         "i.redd.it",
         "3"
        ],
        [
         "3",
         "1afgvv9",
         "Friendly reminder not to work too hard. You'll just get fired",
         "https://www.reddit.com/r/datascience/comments/1afgvv9/friendly_reminder_not_to_work_too_hard_youll_just/",
         null,
         "https://www.reddit.com/r/datascience/comments/1afgvv9/friendly_reminder_not_to_work_too_hard_youll_just/",
         "2024-01-31 12:37:25",
         "1704",
         "0.93",
         "1704",
         "204",
         "False",
         "True",
         "[deleted]",
         null,
         null,
         "0",
         "self.datascience",
         "4"
        ]
       ],
       "shape": {
        "columns": 18,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>permalink</th>\n",
       "      <th>post_hint</th>\n",
       "      <th>url</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>ups</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>is_self</th>\n",
       "      <th>author</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>content_categories</th>\n",
       "      <th>edited</th>\n",
       "      <th>domain</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1f6ztk4</td>\n",
       "      <td>How to avoid 1/2-assed data analysis</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>image</td>\n",
       "      <td>https://i.redd.it/l6zd3zsa9cmd1.jpeg</td>\n",
       "      <td>2024-09-02 07:18:04</td>\n",
       "      <td>3122</td>\n",
       "      <td>0.98</td>\n",
       "      <td>3122</td>\n",
       "      <td>56</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>BdR76</td>\n",
       "      <td>t2_sejrw</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1feyx6h</td>\n",
       "      <td>Favourite piece of code ü§£</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>image</td>\n",
       "      <td>https://i.redd.it/jqvbe1ycrcod1.jpeg</td>\n",
       "      <td>2024-09-12 11:06:40</td>\n",
       "      <td>2810</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2810</td>\n",
       "      <td>102</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>nobody_undefined</td>\n",
       "      <td>t2_dh9o0mnfv</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1dsnbww</td>\n",
       "      <td>You're not helping, Excel! please STOP HELPING!!!</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>image</td>\n",
       "      <td>https://i.redd.it/ccxo0ajs3v9d1.jpeg</td>\n",
       "      <td>2024-07-01 08:46:27</td>\n",
       "      <td>1808</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1808</td>\n",
       "      <td>155</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>BdR76</td>\n",
       "      <td>t2_sejrw</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1afgvv9</td>\n",
       "      <td>Friendly reminder not to work too hard. You'll...</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>2024-01-31 12:37:25</td>\n",
       "      <td>1704</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1704</td>\n",
       "      <td>204</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>self.datascience</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title  \\\n",
       "0  1f6ztk4               How to avoid 1/2-assed data analysis   \n",
       "1  1feyx6h                          Favourite piece of code ü§£   \n",
       "2  1dsnbww  You're not helping, Excel! please STOP HELPING!!!   \n",
       "3  1afgvv9  Friendly reminder not to work too hard. You'll...   \n",
       "\n",
       "                                           permalink post_hint  \\\n",
       "0  https://www.reddit.com/r/datascience/comments/...     image   \n",
       "1  https://www.reddit.com/r/datascience/comments/...     image   \n",
       "2  https://www.reddit.com/r/datascience/comments/...     image   \n",
       "3  https://www.reddit.com/r/datascience/comments/...      None   \n",
       "\n",
       "                                                 url         created_utc  \\\n",
       "0               https://i.redd.it/l6zd3zsa9cmd1.jpeg 2024-09-02 07:18:04   \n",
       "1               https://i.redd.it/jqvbe1ycrcod1.jpeg 2024-09-12 11:06:40   \n",
       "2               https://i.redd.it/ccxo0ajs3v9d1.jpeg 2024-07-01 08:46:27   \n",
       "3  https://www.reddit.com/r/datascience/comments/... 2024-01-31 12:37:25   \n",
       "\n",
       "    ups  upvote_ratio  score  num_comments  is_original_content  is_self  \\\n",
       "0  3122          0.98   3122            56                False    False   \n",
       "1  2810          0.98   2810           102                False    False   \n",
       "2  1808          0.97   1808           155                False    False   \n",
       "3  1704          0.93   1704           204                False     True   \n",
       "\n",
       "             author author_fullname content_categories edited  \\\n",
       "0             BdR76        t2_sejrw               None      0   \n",
       "1  nobody_undefined    t2_dh9o0mnfv               None      0   \n",
       "2             BdR76        t2_sejrw               None      0   \n",
       "3         [deleted]            None               None      0   \n",
       "\n",
       "             domain  rank  \n",
       "0         i.redd.it     1  \n",
       "1         i.redd.it     2  \n",
       "2         i.redd.it     3  \n",
       "3  self.datascience     4  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_db.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. A more principled approach\n",
    "\n",
    "While `to_sql()` is easy and convenient, it is NOT the most efficient way to store data in a database.\n",
    "\n",
    "Similar to Pandas, each column of a SQL table has a data type. When you use `to_sql()`, Pandas will try to infer the data type of each column but it will always default to a 'safe' data type. This can be inefficient if you know the data types of your columns could be more specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the data types of my DataFrame in Pandas?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 18 columns):\n",
      " #   Column               Non-Null Count  Dtype         \n",
      "---  ------               --------------  -----         \n",
      " 0   id                   1000 non-null   object        \n",
      " 1   title                1000 non-null   object        \n",
      " 2   permalink            1000 non-null   object        \n",
      " 3   post_hint            159 non-null    object        \n",
      " 4   url                  1000 non-null   object        \n",
      " 5   created_utc          1000 non-null   datetime64[ns]\n",
      " 6   ups                  1000 non-null   int64         \n",
      " 7   upvote_ratio         1000 non-null   float64       \n",
      " 8   score                1000 non-null   int64         \n",
      " 9   num_comments         1000 non-null   int64         \n",
      " 10  is_original_content  1000 non-null   bool          \n",
      " 11  is_self              1000 non-null   bool          \n",
      " 12  author               1000 non-null   object        \n",
      " 13  author_fullname      924 non-null    object        \n",
      " 14  content_categories   0 non-null      object        \n",
      " 15  edited               1000 non-null   object        \n",
      " 16  domain               1000 non-null   object        \n",
      " 17  rank                 1000 non-null   int64         \n",
      "dtypes: bool(2), datetime64[ns](1), float64(1), int64(4), object(10)\n",
      "memory usage: 127.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might remember from classes past that the 'object' data type in Pandas is a catch-all for any data type that is not a number or a boolean. Typically, this means that the data type is a string.\n",
    "\n",
    "In SQL, you can specify the [data type](https://www.w3schools.com/sql/sql_datatypes.asp) of each column. Confusingly, SQLite has its own data types that are similar to other SQL databases but not exactly the same.\n",
    "\n",
    "<font style=\"color:#e26a4f;font-weight:bold;\">Click [here](https://www.sqlite.org/datatype3.html) to learn more about SQLite-specific data types.</font>\n",
    "\n",
    "üí° **There are several data types to represent strings.** \n",
    "\n",
    "- There is a data type called `TEXT` that can store text of any length. Pandas will always default to `TEXT` when you use `to_sql()`.\n",
    "\n",
    "- But if you _know_ that your text always have a pre-determined number of characters, you can use `CHAR(N)` where `N` is the number of characters. This will be more efficient in terms of storage.\n",
    "\n",
    "- If you _know_ that your text can vary in length but you have an upper limit, you can use `VARCHAR(N)` where `N` is the maximum number of characters.\n",
    "\n",
    "**Similarly, there are several data types to represent numbers, and there are specific data types for dates and times, too.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Check the limits of my data\n",
    "\n",
    "Practice using specific data types by checking the limits of your data.\n",
    "\n",
    "Here is what I suggest you do:\n",
    "\n",
    "- Check all the integers in your DataFrame and establish the minimum and maximum values. Should I store it as `INT2`, `INT8`, `TINYINT`, `BIGINT`, etc.?\n",
    "\n",
    "- Don't worry too much about floating-point numbers. Feel free to just use `REAL` or `FLOAT` if you have numbers with decimal points.\n",
    "\n",
    "- Check all the strings in your DataFrame and establish the maximum length of each string. Should I store it as `CHAR(N)`, `VARCHAR(N)`, or `TEXT`?\n",
    "\n",
    "<span style=\"display:block;background-color:rgba(93, 158, 188, 0.1);padding:0.5em;font-size:1.05em;margin-left:0em;margin-bottom:1em;border-radius:0.5em;width:60%\">ü§î **Think about it:** Sometimes the decision is not about what is the min and max value in your dataset but what is the **theoretical minimum and maximum**. If you choose a data type that is too narrow and later on you need to add a value that goes beyond the limits of that column, you will get an error.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My integer columns\n",
    "int_columns = [col for col in df.columns if df[col].dtype == 'int64']\n",
    "\n",
    "# My float columns\n",
    "float_columns = [col for col in df.columns if df[col].dtype == 'float64']\n",
    "\n",
    "# My text columns (potentially)\n",
    "text_columns = [col for col in df.columns if df[col].dtype == 'object']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking my integers\n",
    "\n",
    "Checking the limits of the integers in my DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ups</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3122.0</td>\n",
       "      <td>3122.0</td>\n",
       "      <td>641.0</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ups   score  num_comments    rank\n",
       "min    19.0    19.0           2.0     1.0\n",
       "max  3122.0  3122.0         641.0  1000.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[int_columns].describe().loc[['min', 'max'],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this, I decided on the following data types:\n",
    "\n",
    "- `ups`: SMALLINT (it seems unlikely that this number will ever be larger than 32,767)\n",
    "\n",
    "- `score`: SMALLINT\n",
    "\n",
    "- `num_comments`: SMALLINT\n",
    "\n",
    "- `rank`: UNSIGNED BIG INT (I feel greedy, so I might want to store more than 4 billion posts in the future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking my strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>permalink</th>\n",
       "      <th>post_hint</th>\n",
       "      <th>url</th>\n",
       "      <th>author</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>content_categories</th>\n",
       "      <th>edited</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  title  permalink  post_hint    url  author  author_fullname  \\\n",
       "min  7.0    9.0       64.0        4.0   20.0     4.0              8.0   \n",
       "max  7.0  295.0      105.0       10.0  188.0    20.0             13.0   \n",
       "\n",
       "    content_categories  edited  domain  \n",
       "min                NaN     NaN     0.0  \n",
       "max                NaN     NaN    27.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[text_columns].apply(lambda x: x.str.len().describe()).loc[['min', 'max'],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above I decided that I will remove the `content_categories` and `edited` columns from my DataFrame, they do not seem to be very useful.\n",
    "\n",
    "I have also decided on the following data types for the remaining columns:\n",
    "\n",
    "- `id`: CHAR(6) (I know that Reddit post IDs are **always** 6 characters long)\n",
    "\n",
    "- `title`: VARCHAR(300) (I don't know the maximum length of a Reddit post title, but it's fairly safe to assume it will be less than 300 characters)\n",
    "\n",
    "- `permalink`: TEXT (links can get long, so I'd rather not risk cutting them off)\n",
    "\n",
    "- `post_hint`: VARCHAR(20) (I know that the post_hint column can only have a few values, so I can limit the length of the string)\n",
    "\n",
    "- `url`: TEXT\n",
    "\n",
    "- `author`: VARCHAR(50) (I know that Reddit usernames can only have a few values, so I can limit the length of the string)\n",
    "\n",
    "- `author_fullname`: VARCHAR(100) (I don't know the maximum length of a Reddit author fullname, but it's fairly safe to assume it will be less than 100 characters)\n",
    "\n",
    "- `domain`: VARCHAR(100) (I'll limit but not too much)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['content_categories', 'edited'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 SQL's CREATE TABLE statement\n",
    "\n",
    "It is best to design your table first, using [SQL's own language](https://www.w3schools.com/sql/) and then insert the data into it. This way, you can specify the data type of each column.\n",
    "\n",
    "Here is an example of how you can create a table in SQLite while specifying the data type of each column:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_statement = text(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS top_posts (\n",
    "    id CHAR(6) PRIMARY KEY,\n",
    "    title VARCHAR(300),\n",
    "    permalink TEXT,\n",
    "    post_hint VARCHAR(20),\n",
    "    url TEXT,\n",
    "    created_utc DATETIME,\n",
    "    ups SMALLINT,\n",
    "    upvote_ratio FLOAT,\n",
    "    score SMALLINT,\n",
    "    num_comments SMALLINT,\n",
    "    is_original_content BOOLEAN,\n",
    "    is_self BOOLEAN,\n",
    "    author VARCHAR(50),\n",
    "    author_fullname VARCHAR(100),\n",
    "    content_categories TEXT,\n",
    "    edited BOOLEAN,\n",
    "    domain VARCHAR(100),\n",
    "    rank UNSIGNED BIG INT\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Now we can create the table\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text('DROP TABLE IF EXISTS top_posts;'))\n",
    "    conn.execute(create_statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Inserting data into an existing table\n",
    "\n",
    "After the code above, the table will be there but it will be empty. We can now use the `pd.to_sql()` function to insert the data into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's insert the data into the table\n",
    "df.to_sql('top_posts', engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Adding a different table\n",
    "\n",
    "If I decide to store the comments in a different table, I can create a new table for them:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE comments (\n",
    "    id CHAR(6) PRIMARY KEY,\n",
    "    post_id CHAR(6),\n",
    "    author VARCHAR(50),\n",
    "    body TEXT,\n",
    "    score SMALLINT,\n",
    "    FOREIGN KEY (post_id) REFERENCES posts(id)\n",
    ");\n",
    "```\n",
    "\n",
    "The [foreign key](https://www.w3schools.com/sql/sql_foreignkey.asp) constraint ensures that the `post_id` in the `comments` table must reference an existing `id` in the `posts` table. This makes merging (joining) the two tables easier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_comment_table = text(\"\"\"\n",
    "    CREATE TABLE comments (\n",
    "        id CHAR(6) PRIMARY KEY,\n",
    "        post_id CHAR(6),\n",
    "        author VARCHAR(50),\n",
    "        body TEXT,\n",
    "        score SMALLINT,\n",
    "        FOREIGN KEY (post_id) REFERENCES posts(id)\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(create_comment_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we added a new table to the exact same database file. No need for multiple CSV files or JSON files. Everything is in one place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Summary\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "- Revisited JSON normalisation using `pd.json_normalize()`\n",
    "\n",
    "- Discussed how to decide which columns to keep in a DataFrame\n",
    "\n",
    "- Learned how to store data in a SQLite database using `pd.to_sql()`\n",
    "\n",
    "- Learned how to create a table in a SQLite database using SQL's `CREATE TABLE` statement\n",
    "\n",
    "- Learned how to insert data into an existing table in a SQLite database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. What's Next?\n",
    "\n",
    "Next week we will discuss how to think deeper about the structure of your data, how to link up tables with PRIMARY and FOREIGN KEYS, and how to query data from a database using SQL.\n",
    "\n",
    "We will also discuss best practices for data visualisation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
